{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T10:18:39.199865800Z",
     "start_time": "2024-11-25T10:13:15.658834Z"
    }
   },
   "source": [
    "import sys\n",
    "import typer\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def random_sign_(tensor: torch.Tensor, prob: float = 0.5, value: float = 1.0):\n",
    "    \"\"\"\n",
    "    Randomly set elements of the input tensor to either +value or -value.\n",
    "    \"\"\"\n",
    "    sign = torch.where(torch.rand_like(tensor) < prob, 1.0, -1.0)\n",
    "    with torch.no_grad():\n",
    "        tensor.copy_(sign * value)\n",
    "\n",
    "class BatchEnsembleMixin:\n",
    "    def init_ensemble(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        ensemble_size: int,\n",
    "        alpha_init: float | None = None,\n",
    "        gamma_init: float | None = None,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.alpha_init = alpha_init\n",
    "        self.gamma_init = gamma_init\n",
    "\n",
    "        if not isinstance(self, nn.Module):\n",
    "            raise TypeError(\"BatchEnsembleMixin must be mixed with nn.Module or one of its subclasses\")\n",
    "\n",
    "        if alpha_init is None:\n",
    "            self.register_parameter(\"alpha_param\", None)\n",
    "        else:\n",
    "            self.alpha_param = self.init_scaling_parameter(alpha_init, in_features, device=device, dtype=dtype)\n",
    "            self.register_parameter(\"alpha_param\", self.alpha_param)\n",
    "\n",
    "        if gamma_init is None:\n",
    "            self.register_parameter(\"gamma_param\", None)\n",
    "        else:\n",
    "            self.gamma_param = self.init_scaling_parameter(gamma_init, out_features, device=device, dtype=dtype)\n",
    "            self.register_parameter(\"gamma_param\", self.gamma_param)\n",
    "\n",
    "        if bias:\n",
    "            self.bias_param = nn.Parameter(torch.zeros(ensemble_size, out_features, device=device, dtype=dtype))\n",
    "            self.register_parameter(\"bias_param\", self.bias_param)\n",
    "        else:\n",
    "            self.register_parameter(\"bias_param\", None)\n",
    "\n",
    "    def init_scaling_parameter(self, init_value: float, num_features: int, device=None, dtype=None):\n",
    "        param = torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype)\n",
    "        if init_value < 0:\n",
    "            param.normal_(mean=1, std=-init_value)\n",
    "        else:\n",
    "            random_sign_(param, prob=init_value, value=1.0)\n",
    "        return nn.Parameter(param)\n",
    "\n",
    "    def expand_param(self, x: torch.Tensor, param: torch.Tensor):\n",
    "        num_repeats = x.size(0) // self.ensemble_size\n",
    "        expanded_param = torch.repeat_interleave(param, num_repeats, dim=0)\n",
    "        extra_dims = len(x.shape) - len(expanded_param.shape)\n",
    "        for _ in range(extra_dims):\n",
    "            expanded_param = expanded_param.unsqueeze(-1)\n",
    "        return expanded_param\n",
    "\n",
    "class BELinear(nn.Linear, BatchEnsembleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        ensemble_size: int = 1,\n",
    "        alpha_init: float | None = None,\n",
    "        gamma_init: float | None = None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias=False, device=device, dtype=dtype)\n",
    "        self.init_ensemble(in_features, out_features, ensemble_size, alpha_init, gamma_init, bias)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if self.alpha_init is not None:\n",
    "            input = input * self.expand_param(input, self.alpha_param)\n",
    "        x = F.linear(input, self.weight)\n",
    "        if self.gamma_init is not None:\n",
    "            x = x * self.expand_param(x, self.gamma_param)\n",
    "        if self.bias_param is not None:\n",
    "            x = x + self.expand_param(x, self.bias_param)\n",
    "        return x\n",
    "\n",
    "class Conv2d(nn.Conv2d, BatchEnsembleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias: bool = True,\n",
    "        padding_mode='zeros',\n",
    "        ensemble_size: int = 1,\n",
    "        alpha_init: float | None = None,\n",
    "        gamma_init: float | None = None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,  # Bias is managed separately\n",
    "            padding_mode=padding_mode,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.init_ensemble(\n",
    "            in_features=in_channels,\n",
    "            out_features=out_channels,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha_init,  # Corrected from alpha_gamma_init\n",
    "            gamma_init=gamma_init,  # Corrected from alpha_gamma_init\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if self.alpha_init is not None:\n",
    "            input = input * self.expand_param(input, self.alpha_param)\n",
    "        x = self._conv_forward(input, self.weight, None)\n",
    "        if self.gamma_init is not None:\n",
    "            x = x * self.expand_param(x, self.gamma_param)\n",
    "        if self.bias_param is not None:\n",
    "            x = x + self.expand_param(x, self.bias_param)\n",
    "        return x\n",
    "\n",
    "class BatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps=1e-5,\n",
    "        momentum=0.1,\n",
    "        affine=False,  # We will manage affine parameters ourselves\n",
    "        track_running_stats=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        ensemble_size: int = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_features,\n",
    "            eps=eps,\n",
    "            momentum=momentum,\n",
    "            affine=affine,\n",
    "            track_running_stats=track_running_stats,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.ensemble_size = ensemble_size\n",
    "        # Correct tensor initialization\n",
    "        self.weight_be = nn.Parameter(torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype))\n",
    "        self.bias_be = nn.Parameter(torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype))\n",
    "        self.reset_be_parameters()\n",
    "\n",
    "    def reset_be_parameters(self):\n",
    "        nn.init.ones_(self.weight_be)\n",
    "        nn.init.zeros_(self.bias_be)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # Use the base class's forward method\n",
    "        x = super().forward(input)\n",
    "        num_repeats = x.size(0) // self.ensemble_size\n",
    "        weight = torch.repeat_interleave(self.weight_be, num_repeats, dim=0).unsqueeze(2).unsqueeze(3)\n",
    "        bias = torch.repeat_interleave(self.bias_be, num_repeats, dim=0).unsqueeze(2).unsqueeze(3)\n",
    "        x = x * weight + bias\n",
    "        return x\n",
    "\n",
    "# StandardScaler\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Based on https://gist.github.com/farahmand-m/8a416f33a27d73a149f92ce4708beb40\n",
    "    Extended with inverse_transform\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mean: torch.Tensor | None = None,\n",
    "        std: torch.Tensor | None = None,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, data: torch.Tensor):\n",
    "        reduction_axes = list(range(data.dim() - 1))\n",
    "        self.mean = torch.mean(data, dim=reduction_axes)\n",
    "        self.std = torch.std(data, dim=reduction_axes)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise RuntimeError(\"Call fit before calling transform.\")\n",
    "        scaled_mean = (data - self.mean) / (self.std + self.epsilon)\n",
    "        return scaled_mean\n",
    "\n",
    "    def inverse_transform(self, scaled_data: torch.Tensor) -> torch.Tensor:\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise RuntimeError(\"Call fit before calling inverse_transform.\")\n",
    "        unscaled_mean = scaled_data * (self.std + self.epsilon) + self.mean\n",
    "        return unscaled_mean\n",
    "\n",
    "    def fit_transform(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def to(self, target_device: torch.device):\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            self.mean = self.mean.to(target_device)\n",
    "            self.std = self.std.to(target_device)\n",
    "        return self\n",
    "\n",
    "# MLP Models for MNIST\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size=256, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SharedParametersMLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size=256, num_classes=10, num_heads=4):\n",
    "        super(SharedParametersMLP, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Multiple heads for classification\n",
    "        self.heads = nn.ModuleList([nn.Linear(hidden_size, num_classes) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        # Collect outputs from all heads\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        outputs = torch.stack(outputs)  # Shape: [num_heads, batch_size, num_classes]\n",
    "        # Average the outputs over heads\n",
    "        x = outputs.mean(dim=0)  # Shape: [batch_size, num_classes]\n",
    "        return x\n",
    "\n",
    "class BatchEnsembleMLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size=256, num_classes=10, ensemble_size=4, alpha=0.5, gamma=0.5):\n",
    "        super(BatchEnsembleMLP, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.fc1 = BELinear(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma,\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = BELinear(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = BELinear(\n",
    "            hidden_size,\n",
    "            num_classes,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SharedParametersBatchEnsembleMLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size=256, num_classes=10, ensemble_size=4, alpha=0.5, gamma=0.5):\n",
    "        super(SharedParametersBatchEnsembleMLP, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Shared BatchEnsemble layers\n",
    "        self.fc1 = BELinear(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma,\n",
    "        )\n",
    "        self.fc2 = BELinear(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma,\n",
    "        )\n",
    "\n",
    "        # Multiple heads for classification\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, num_classes) for _ in range(ensemble_size)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Pass through shared BatchEnsemble layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "\n",
    "        # Reshape x to [ensemble_size, batch_size, hidden_size]\n",
    "        batch_size = x.size(0) // self.ensemble_size\n",
    "        x = x.view(self.ensemble_size, batch_size, -1)\n",
    "\n",
    "        # Apply each head and collect outputs\n",
    "        outputs = []\n",
    "        for i, head in enumerate(self.heads):\n",
    "            out = head(x[i])  # x[i] is [batch_size, hidden_size]\n",
    "            outputs.append(out)\n",
    "\n",
    "        # Stack outputs: Shape [ensemble_size, batch_size, num_classes]\n",
    "        outputs = torch.stack(outputs)  # Shape: [ensemble_size, batch_size, num_classes]\n",
    "\n",
    "        # Average over ensemble members\n",
    "        outputs = outputs.mean(dim=0)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# CNN Models\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.num_classes = 10\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(64 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # [batch_size, 32, 28, 28]\n",
    "        x = self.pool(x)                        # [batch_size, 32, 14, 14]\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # [batch_size, 64, 14, 14]\n",
    "        x = self.pool(x)                        # [batch_size, 64, 7, 7]\n",
    "        x = x.view(x.size(0), -1)               # [batch_size, 64*7*7]\n",
    "        x = self.fc(x)                          # [batch_size, 10]\n",
    "        return x\n",
    "\n",
    "class SharedParametersCNN(nn.Module):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(SharedParametersCNN, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = 10\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.shared_fc = nn.Linear(64 * 7 * 7, 128)\n",
    "        # Multiple heads for classification\n",
    "        self.heads = nn.ModuleList([nn.Linear(128, 10) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # [batch_size, 32, 28, 28]\n",
    "        x = self.pool(x)                        # [batch_size, 32, 14, 14]\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # [batch_size, 64, 14, 14]\n",
    "        x = self.pool(x)                        # [batch_size, 64, 7, 7]\n",
    "        x = x.view(x.size(0), -1)               # [batch_size, 64*7*7]\n",
    "        x = self.shared_fc(x)                   # [batch_size, 128]\n",
    "        # Collect outputs from all heads\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        outputs = torch.stack(outputs)           # Shape: [num_heads, batch_size, num_classes]\n",
    "        # Average the outputs over heads\n",
    "        x = outputs.mean(dim=0)                  # Shape: [batch_size, num_classes]\n",
    "        return x\n",
    "\n",
    "class BatchEnsembleCNN(nn.Module):\n",
    "    def __init__(self, ensemble_size=4, alpha=0.5, gamma=0.5):\n",
    "        super(BatchEnsembleCNN, self).__init__()\n",
    "        self.num_classes = 10\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=3, padding=1,\n",
    "                            ensemble_size=ensemble_size,\n",
    "                            alpha_init=alpha,\n",
    "                            gamma_init=gamma)\n",
    "        self.bn1 = BatchNorm2d(32, ensemble_size=ensemble_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3, padding=1,\n",
    "                            ensemble_size=ensemble_size,\n",
    "                            alpha_init=alpha,\n",
    "                            gamma_init=gamma)\n",
    "        self.bn2 = BatchNorm2d(64, ensemble_size=ensemble_size)\n",
    "        self.fc = BELinear(64 * 7 * 7, 10,\n",
    "                           ensemble_size=ensemble_size,\n",
    "                           alpha_init=alpha,\n",
    "                           gamma_init=gamma)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))   # [batch_size * ensemble_size, 32, 28, 28]\n",
    "        x = self.pool(x)                         # [batch_size * ensemble_size, 32, 14, 14]\n",
    "        x = self.relu(self.bn2(self.conv2(x)))   # [batch_size * ensemble_size, 64, 14, 14]\n",
    "        x = self.pool(x)                         # [batch_size * ensemble_size, 64, 7, 7]\n",
    "        x = x.view(x.size(0), -1)                # [batch_size * ensemble_size, 64*7*7]\n",
    "        x = self.fc(x)                           # [batch_size * ensemble_size, 10]\n",
    "        return x\n",
    "\n",
    "class SharedParametersBatchEnsembleCNN(nn.Module):\n",
    "    def __init__(self, ensemble_size=4, alpha=0.5, gamma=0.5):\n",
    "        super(SharedParametersBatchEnsembleCNN, self).__init__()\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.num_classes = 10\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Shared BatchEnsemble convolutional layers\n",
    "        self.conv1 = Conv2d(\n",
    "            1, 32, kernel_size=3, padding=1,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma\n",
    "        )\n",
    "        self.bn1 = BatchNorm2d(32, ensemble_size=ensemble_size)\n",
    "        self.conv2 = Conv2d(\n",
    "            32, 64, kernel_size=3, padding=1,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma\n",
    "        )\n",
    "        self.bn2 = BatchNorm2d(64, ensemble_size=ensemble_size)\n",
    "        \n",
    "        # Shared fully connected layer\n",
    "        self.shared_fc = BELinear(\n",
    "            64 * 7 * 7, 128,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha,\n",
    "            gamma_init=gamma\n",
    "        )\n",
    "        \n",
    "        # Multiple heads for classification\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(128, self.num_classes) for _ in range(ensemble_size)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shared convolutional layers\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # Shape: [batch_size * ensemble_size, 32, 28, 28]\n",
    "        x = self.pool(x)                        # Shape: [batch_size * ensemble_size, 32, 14, 14]\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # Shape: [batch_size * ensemble_size, 64, 14, 14]\n",
    "        x = self.pool(x)                        # Shape: [batch_size * ensemble_size, 64, 7, 7]\n",
    "        x = x.view(x.size(0), -1)               # Shape: [batch_size * ensemble_size, 64*7*7]\n",
    "\n",
    "        # Shared fully connected layer\n",
    "        x = self.shared_fc(x)                   # Shape: [batch_size * ensemble_size, 128]\n",
    "\n",
    "        # Reshape x to [ensemble_size, batch_size, 128]\n",
    "        batch_size = x.size(0) // self.ensemble_size\n",
    "        x = x.view(self.ensemble_size, batch_size, -1)\n",
    "\n",
    "        # Apply each head and collect outputs\n",
    "        outputs = []\n",
    "        for i, head in enumerate(self.heads):\n",
    "            out = head(x[i])  # x[i] is [batch_size, 128]\n",
    "            outputs.append(out)\n",
    "\n",
    "        # Stack outputs: Shape [ensemble_size, batch_size, num_classes]\n",
    "        outputs = torch.stack(outputs)  # Shape: [ensemble_size, batch_size, num_classes]\n",
    "\n",
    "        # Average over ensemble members\n",
    "        outputs = outputs.mean(dim=0)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def train_mnist_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    model_type=\"simple\",\n",
    "    ensemble_size=4,\n",
    "    num_epochs=15,\n",
    "    lr=0.0002,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    model.to(device)  # Move the model to the GPU if available\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if model_type == \"batchensemble\":\n",
    "                # Repeat images and labels for BatchEnsemble\n",
    "                images = images.repeat(ensemble_size, 1, 1, 1)\n",
    "                labels = labels.repeat(ensemble_size)\n",
    "            elif model_type == \"shared_batchensemble\":\n",
    "                # Repeat images only\n",
    "                images = images.repeat(ensemble_size, 1, 1, 1)\n",
    "                # Do not repeat labels\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                if model_type == \"batchensemble\":\n",
    "                    images = images.repeat(ensemble_size, 1, 1, 1)\n",
    "                    outputs = model(images)\n",
    "                    # Reshape and average over ensemble instances\n",
    "                    outputs = outputs.view(ensemble_size, -1, model.num_classes)\n",
    "                    outputs = outputs.mean(dim=0)\n",
    "                elif model_type == \"shared_batchensemble\":\n",
    "                    images = images.repeat(ensemble_size, 1, 1, 1)\n",
    "                    outputs = model(images)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], {model_type.replace(\"_\", \" \").capitalize()} Model Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, test_accuracies\n",
    "    \n",
    "def plot_training_results(train_losses, test_accuracies, ax, model_type=\"simple\"):\n",
    "    ax.plot(train_losses, label=\"Train loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"{model_type.replace('_', ' ').capitalize()} Training Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(test_accuracies, label=\"Test Accuracy\", color=\"orange\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\")\n",
    "    ax2.legend(loc='lower right')\n",
    "    \n",
    "@torch.no_grad()\n",
    "def plot_parameters(model, model_name=\"Model\", plot_type=\"kde\", ax=None):\n",
    "    \"\"\"\n",
    "    Plots the distribution of model parameters using different plot types.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model whose parameters are to be plotted.\n",
    "    - model_name (str): A name for the model to be displayed in the plot titles.\n",
    "    - plot_type (str): The type of plot to generate. Options are:\n",
    "        - 'kde': Kernel Density Estimate plot.\n",
    "        - 'histogram': Combined histogram for all parameter types.\n",
    "        - 'facet_hist': FacetGrid of histograms separated by parameter type.\n",
    "    - ax (matplotlib.axes.Axes, optional): The axes on which to plot. Required for 'kde' and 'histogram' types.\n",
    "                                           Not used for 'facet_hist' as it creates its own figure.\n",
    "    \"\"\"\n",
    "    # Collect all parameters and categorize them by type\n",
    "    params = []\n",
    "    tags = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" in name:\n",
    "            tag = \"bias\"\n",
    "        elif \"alpha\" in name:\n",
    "            tag = \"alpha\"\n",
    "        elif \"gamma\" in name:\n",
    "            tag = \"gamma\"\n",
    "        elif \"weight\" in name:\n",
    "            tag = \"weight\"\n",
    "        else:\n",
    "            tag = \"other\"\n",
    "        params.append(param.detach().cpu().flatten())\n",
    "        tags += [tag] * param.numel()\n",
    "    \n",
    "    # Concatenate all parameters into a single array\n",
    "    params = torch.cat(params).numpy()\n",
    "    tags = np.array(tags)\n",
    "    \n",
    "    # Create a DataFrame for seaborn\n",
    "    df = pd.DataFrame({\n",
    "        'Parameter Value': params,\n",
    "        'Type': tags\n",
    "    })\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean = np.mean(params)\n",
    "    std = np.std(params)\n",
    "    median = np.median(params)\n",
    "    \n",
    "    # Plot based on the specified plot type\n",
    "    if plot_type == \"kde\":\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.kdeplot(\n",
    "            data=df,\n",
    "            x='Parameter Value',\n",
    "            hue='Type',\n",
    "            ax=ax,\n",
    "            fill=True,\n",
    "            common_norm=False,  # Normalize each KDE separately\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.set_xlabel(\"Parameter Value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Parameter Distribution (KDE) of {model_name}\")\n",
    "        # Display statistics\n",
    "        ax.text(0.95, 0.95, f\"Mean: {mean:.2e}\\nStd: {std:.2e}\\nMedian: {median:.2e}\",\n",
    "                transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    elif plot_type == \"histogram\":\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.histplot(\n",
    "            data=df,\n",
    "            x='Parameter Value',\n",
    "            hue='Type',\n",
    "            multiple='stack',\n",
    "            bins=50,\n",
    "            kde=False,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_xlabel(\"Parameter Value\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(f\"Parameter Distribution (Histogram) of {model_name}\")\n",
    "        # Display statistics\n",
    "        ax.text(0.95, 0.95, f\"Mean: {mean:.2e}\\nStd: {std:.2e}\\nMedian: {median:.2e}\",\n",
    "                transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    elif plot_type == \"facet_hist\":\n",
    "        # Create a FacetGrid of histograms separated by parameter type\n",
    "        g = sns.FacetGrid(df, col=\"Type\", col_wrap=2, sharex=False, sharey=False, height=4)\n",
    "        g.map_dataframe(sns.histplot, x=\"Parameter Value\", bins=50, kde=False, color=\"steelblue\")\n",
    "        g.set_axis_labels(\"Parameter Value\", \"Count\")\n",
    "        g.fig.suptitle(f\"Parameter Distribution (FacetGrid Histogram) of {model_name}\", y=1.02)\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        # Display statistics on the first subplot\n",
    "        if len(g.axes.flat) > 0:\n",
    "            ax0 = g.axes.flat[0]\n",
    "            ax0.text(0.95, 0.95, f\"Mean: {mean:.2e}\\nStd: {std:.2e}\\nMedian: {median:.2e}\",\n",
    "                     transform=ax0.transAxes, fontsize=10,\n",
    "                     verticalalignment='top', horizontalalignment='right',\n",
    "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type. Choose from 'kde', 'histogram', or 'facet_hist'.\")\n",
    "\n",
    "\n",
    "app = typer.Typer()\n",
    "\n",
    "def train_with_params(\n",
    "    seed: int = typer.Option(42, help=\"Size of the seed\"),   \n",
    "    ensemble_size: int = typer.Option(4, help=\"Size of the ensemble\"),\n",
    "    alpha_init: float = typer.Option(0.5, help=\"Initial value for Alpha in BatchEnsemble\"),\n",
    "    gamma_init: float = typer.Option(0.5, help=\"Initial value for Gamma in BatchEnsemble\"),\n",
    "    num_epochs: int = typer.Option(15, help=\"Number of training epochs\"),\n",
    "    batch_size: int = typer.Option(256, help=\"Batch size for training\"),\n",
    "    lr: float = typer.Option(0.0002, help=\"Learning rate for the optimizer\"),\n",
    "    device: str = typer.Option(\"cpu\", help=\"Device to use, 'cuda' for GPU or 'cpu' for CPU\")\n",
    "):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    num_heads = ensemble_size  # For consistency\n",
    "    \n",
    "    \n",
    "        # Ensure that batch_size is divisible by ensemble_size\n",
    "    if batch_size % ensemble_size != 0:\n",
    "        raise ValueError(\"Batch size must be divisible by ensemble size.\")\n",
    "\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Define model types\n",
    "    model_types = [\"simple\", \"batchensemble\", \"sharedparameters\", \"shared_batchensemble\"]\n",
    "\n",
    "    # Initialize dictionaries to store models and their metrics\n",
    "    mlp_models = {}\n",
    "    mlp_train_losses = {}\n",
    "    mlp_test_accuracies = {}\n",
    "\n",
    "    # Train and evaluate MLP models\n",
    "    for model_type in model_types:\n",
    "        if model_type == \"simple\":\n",
    "            model = MLP().to(device)\n",
    "            ensemble_size_mlp = 1\n",
    "        elif model_type == \"batchensemble\":\n",
    "            model = BatchEnsembleMLP(\n",
    "                ensemble_size=ensemble_size,\n",
    "                alpha=alpha_init,\n",
    "                gamma=gamma_init\n",
    "            ).to(device)\n",
    "            ensemble_size_mlp = ensemble_size\n",
    "        elif model_type == \"sharedparameters\":\n",
    "            model = SharedParametersMLP(\n",
    "                num_heads=num_heads\n",
    "            ).to(device)\n",
    "            ensemble_size_mlp = 1  # Shared parameters handle multiple heads internally\n",
    "        elif model_type == \"shared_batchensemble\":\n",
    "            model = SharedParametersBatchEnsembleMLP(\n",
    "                ensemble_size=ensemble_size, \n",
    "                alpha=alpha_init, \n",
    "                gamma=gamma_init\n",
    "            ).to(device)\n",
    "            ensemble_size_mlp = ensemble_size\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type.\")\n",
    "        mlp_models[model_type] = model\n",
    "        print(f\"Training MLP model: {model_type}\")\n",
    "        tr_losses, te_accuracies = train_mnist_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            model_type=model_type,\n",
    "            ensemble_size=ensemble_size_mlp,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            device=device,  # Ensure device is passed\n",
    "        )\n",
    "        mlp_train_losses[model_type] = tr_losses\n",
    "        mlp_test_accuracies[model_type] = te_accuracies\n",
    "\n",
    "    # Plot MLP training results and parameter distributions\n",
    "    # Define plot types\n",
    "    plot_types = [\"kde\", \"histogram\"]  # Add \"facet_hist\" if desired\n",
    "\n",
    "    # Create a figure with subplots for training results and parameter plots\n",
    "    num_models = len(model_types)\n",
    "    num_plot_types = len(plot_types)\n",
    "    fig, axs = plt.subplots(num_plot_types + 1, num_models, figsize=(24, 18))  # +1 for training plots\n",
    "\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        # Plot training loss and test accuracy on the first row\n",
    "        plot_training_results(\n",
    "            mlp_train_losses[model_type],\n",
    "            mlp_test_accuracies[model_type],\n",
    "            axs[0, i],\n",
    "            model_type=model_type\n",
    "        )\n",
    "        # Plot parameter distributions for each plot type\n",
    "        for j, plot_type in enumerate(plot_types):\n",
    "            plot_parameters(\n",
    "                mlp_models[model_type],\n",
    "                model_name=f\"{model_type.replace('_', ' ').capitalize()} MLP\",\n",
    "                plot_type=plot_type,\n",
    "                ax=axs[j + 1, i]\n",
    "            )\n",
    "            # Optional: Remove any additional plot types by commenting them out\n",
    "\n",
    "    # Adjust layout and add a main title\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(\"MLP Models: Training and Parameter Distributions\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Create separate FacetGrid histograms for each MLP model\n",
    "    for model_type in model_types:\n",
    "        plot_parameters(\n",
    "            mlp_models[model_type],\n",
    "            model_name=f\"{model_type.replace('_', ' ').capitalize()} MLP\",\n",
    "            plot_type=\"facet_hist\"\n",
    "        )\n",
    "        plt.show()\n",
    "        # Optional: Comment out the plt.show() if you do not want to display each plot immediately\n",
    "\n",
    "    # Initialize dictionaries to store CNN models and their metrics\n",
    "    cnn_models = {}\n",
    "    cnn_train_losses = {}\n",
    "    cnn_test_accuracies = {}\n",
    "\n",
    "    # Train and evaluate CNN models\n",
    "    for model_type in model_types:\n",
    "        if model_type == \"simple\":\n",
    "            model = SimpleCNN().to(device)\n",
    "            ensemble_size_cnn = 1\n",
    "        elif model_type == \"batchensemble\":\n",
    "            model = BatchEnsembleCNN(\n",
    "                ensemble_size=ensemble_size,\n",
    "                alpha=alpha_init,\n",
    "                gamma=gamma_init\n",
    "            ).to(device)\n",
    "            ensemble_size_cnn = ensemble_size\n",
    "        elif model_type == \"sharedparameters\":\n",
    "            model = SharedParametersCNN(\n",
    "                num_heads=num_heads\n",
    "            ).to(device)\n",
    "            ensemble_size_cnn = 1  # Shared parameters handle multiple heads internally\n",
    "        elif model_type == \"shared_batchensemble\":\n",
    "            model = SharedParametersBatchEnsembleCNN(\n",
    "                ensemble_size=ensemble_size,\n",
    "                alpha=alpha_init,\n",
    "                gamma=gamma_init\n",
    "            ).to(device)\n",
    "            ensemble_size_cnn = ensemble_size\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type.\")\n",
    "        cnn_models[model_type] = model\n",
    "        print(f\"Training CNN model: {model_type}\")\n",
    "        tr_losses, te_accuracies = train_mnist_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            model_type=model_type,\n",
    "            ensemble_size=ensemble_size_cnn,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            device=device,  # Ensure device is passed\n",
    "        )\n",
    "        cnn_train_losses[model_type] = tr_losses\n",
    "        cnn_test_accuracies[model_type] = te_accuracies\n",
    "\n",
    "    # Plot CNN training results and parameter distributions\n",
    "    # Define plot types\n",
    "    plot_types = [\"kde\", \"histogram\"]  # Add \"facet_hist\" if desired\n",
    "\n",
    "    # Create a figure with subplots for training results and parameter plots\n",
    "    num_models = len(model_types)\n",
    "    num_plot_types = len(plot_types)\n",
    "    fig, axs = plt.subplots(num_plot_types + 1, num_models, figsize=(24, 18))  # +1 for training plots\n",
    "\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        # Plot training loss and test accuracy on the first row\n",
    "        plot_training_results(\n",
    "            cnn_train_losses[model_type],\n",
    "            cnn_test_accuracies[model_type],\n",
    "            axs[0, i],\n",
    "            model_type=model_type\n",
    "        )\n",
    "        # Plot parameter distributions for each plot type\n",
    "        for j, plot_type in enumerate(plot_types):\n",
    "            plot_parameters(\n",
    "                cnn_models[model_type],\n",
    "                model_name=f\"{model_type.replace('_', ' ').capitalize()} CNN\",\n",
    "                plot_type=plot_type,\n",
    "                ax=axs[j + 1, i]\n",
    "            )\n",
    "            # Optional: Remove any additional plot types by commenting them out\n",
    "\n",
    "    # Adjust layout and add a main title\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(\"CNN Models: Training and Parameter Distributions\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Create separate FacetGrid histograms for each CNN model\n",
    "    for model_type in model_types:\n",
    "        plot_parameters(\n",
    "            cnn_models[model_type],\n",
    "            model_name=f\"{model_type.replace('_', ' ').capitalize()} CNN\",\n",
    "            plot_type=\"facet_hist\"\n",
    "        )\n",
    "        plt.show()\n",
    "        # Optional: Comment out the plt.show() if you do not want to display each plot immediately\n",
    "\n",
    "@app.command()\n",
    "def run_experiments():\n",
    "    \"\"\"Defines a CLI to test multiple parameter combinations.\"\"\"\n",
    "    alpha_list = [0.5]\n",
    "    gamma_list = [0.5]\n",
    "    ensemble_size_list = [4]\n",
    "    seed_list = [42]\n",
    "    epochs_list = [15]\n",
    "    batch_list = [256]\n",
    "    lr_list = [0.0002]\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for seed in seed_list:\n",
    "        for alpha in alpha_list:\n",
    "            for gamma in gamma_list:\n",
    "                for ensemble_size in ensemble_size_list:\n",
    "                    for epochs in epochs_list:\n",
    "                        for batch_size in batch_list:\n",
    "                            for lr in lr_list:\n",
    "                                train_with_params(seed=seed, ensemble_size=ensemble_size, alpha_init=alpha, gamma_init=gamma, num_epochs=epochs, batch_size=batch_size, lr=lr, device=device)\n",
    "\n",
    "sys.argv = [\"MLP_CNN_BatchEns_MNIST_Parametersharing_Typer.py\"]\n",
    "typer.run(run_experiments)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training MLP model: simple\n",
      "Epoch [1/15], Simple Model Test Accuracy: 90.83%\n",
      "Epoch [2/15], Simple Model Test Accuracy: 92.62%\n",
      "Epoch [3/15], Simple Model Test Accuracy: 93.51%\n",
      "Epoch [4/15], Simple Model Test Accuracy: 94.39%\n",
      "Epoch [5/15], Simple Model Test Accuracy: 95.13%\n",
      "Epoch [6/15], Simple Model Test Accuracy: 95.83%\n",
      "Epoch [7/15], Simple Model Test Accuracy: 96.16%\n",
      "Epoch [8/15], Simple Model Test Accuracy: 96.26%\n",
      "Epoch [9/15], Simple Model Test Accuracy: 96.68%\n",
      "Epoch [10/15], Simple Model Test Accuracy: 96.86%\n",
      "Epoch [11/15], Simple Model Test Accuracy: 96.89%\n",
      "Epoch [12/15], Simple Model Test Accuracy: 97.06%\n",
      "Epoch [13/15], Simple Model Test Accuracy: 97.34%\n",
      "Epoch [14/15], Simple Model Test Accuracy: 97.44%\n",
      "Epoch [15/15], Simple Model Test Accuracy: 97.52%\n",
      "Training MLP model: batchensemble\n",
      "Epoch [1/15], Batchensemble Model Test Accuracy: 89.90%\n",
      "Epoch [2/15], Batchensemble Model Test Accuracy: 91.94%\n",
      "Epoch [3/15], Batchensemble Model Test Accuracy: 93.21%\n",
      "Epoch [4/15], Batchensemble Model Test Accuracy: 93.75%\n",
      "Epoch [5/15], Batchensemble Model Test Accuracy: 94.45%\n",
      "Epoch [6/15], Batchensemble Model Test Accuracy: 95.13%\n",
      "Epoch [7/15], Batchensemble Model Test Accuracy: 95.44%\n",
      "Epoch [8/15], Batchensemble Model Test Accuracy: 95.94%\n",
      "Epoch [9/15], Batchensemble Model Test Accuracy: 96.07%\n",
      "Epoch [10/15], Batchensemble Model Test Accuracy: 96.52%\n",
      "Epoch [11/15], Batchensemble Model Test Accuracy: 96.57%\n",
      "Epoch [12/15], Batchensemble Model Test Accuracy: 96.91%\n",
      "Epoch [13/15], Batchensemble Model Test Accuracy: 96.97%\n",
      "Epoch [14/15], Batchensemble Model Test Accuracy: 97.25%\n",
      "Epoch [15/15], Batchensemble Model Test Accuracy: 97.31%\n",
      "Training MLP model: sharedparameters\n",
      "Epoch [1/15], Sharedparameters Model Test Accuracy: 90.16%\n",
      "Epoch [2/15], Sharedparameters Model Test Accuracy: 92.39%\n",
      "Epoch [3/15], Sharedparameters Model Test Accuracy: 93.78%\n",
      "Epoch [4/15], Sharedparameters Model Test Accuracy: 94.50%\n",
      "Epoch [5/15], Sharedparameters Model Test Accuracy: 95.11%\n",
      "Epoch [6/15], Sharedparameters Model Test Accuracy: 95.72%\n",
      "Epoch [7/15], Sharedparameters Model Test Accuracy: 96.21%\n",
      "Epoch [8/15], Sharedparameters Model Test Accuracy: 96.37%\n",
      "Epoch [9/15], Sharedparameters Model Test Accuracy: 96.63%\n",
      "Epoch [10/15], Sharedparameters Model Test Accuracy: 96.85%\n",
      "Epoch [11/15], Sharedparameters Model Test Accuracy: 96.94%\n",
      "Epoch [12/15], Sharedparameters Model Test Accuracy: 97.16%\n",
      "Epoch [13/15], Sharedparameters Model Test Accuracy: 97.29%\n",
      "Epoch [14/15], Sharedparameters Model Test Accuracy: 97.34%\n",
      "Epoch [15/15], Sharedparameters Model Test Accuracy: 97.43%\n",
      "Training MLP model: shared_batchensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[31mAborted.\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Aborted.</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
