{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOCq0rNvJ37U7P8wyoRkOo/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Paramter Testing and Optimizer Comparison\n","\n","For running this notebook please keep in mind that you need to change some code lines in order to get the desired results.\n","\n","Please be aware of the following possibilities to change:\n","- Select model type (e.g. batchensemble, complexbatchensemble)\n","- Select desired experiment (e.g. alpha and gamma, optimizer)\n","- In the experiments select the desired parameter to train the model"],"metadata":{"id":"QeQ3-n9e_4X-"}},{"cell_type":"code","source":["# Imports\n","\n","import typer\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import torch.optim as optim\n","import os\n","import csv\n","import ivon"],"metadata":{"id":"HYmIXVxRqlav","executionInfo":{"status":"ok","timestamp":1734708849076,"user_tz":-60,"elapsed":17501,"user":{"displayName":"Justine","userId":"10003038839395646781"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VWAsqRmiqazP","executionInfo":{"status":"ok","timestamp":1734708849076,"user_tz":-60,"elapsed":9,"user":{"displayName":"Justine","userId":"10003038839395646781"}}},"outputs":[],"source":["# BatchEnsemble implementation\n","\n","def random_sign_(tensor: torch.Tensor, prob: float = 0.5, value: float = 1.0):\n","    \"\"\"\n","    Randomly set elements of the input tensor to either +value or -value.\n","    \"\"\"\n","    sign = torch.where(torch.rand_like(tensor) < prob, 1.0, -1.0)\n","    with torch.no_grad():\n","        tensor.copy_(sign * value)\n","\n","\n","class BatchEnsembleMixin:\n","    def init_ensemble(\n","            self,\n","            in_features: int,\n","            out_features: int,\n","            ensemble_size: int,\n","            alpha_init: float | None = None,\n","            gamma_init: float | None = None,\n","            bias: bool = True,\n","            device=None,\n","            dtype=None,\n","    ):\n","        self.ensemble_size = ensemble_size\n","        self.alpha_init = alpha_init\n","        self.gamma_init = gamma_init\n","\n","        if not isinstance(self, nn.Module):\n","            raise TypeError(\"BatchEnsembleMixin must be mixed with nn.Module or one of its subclasses\")\n","\n","        if alpha_init is None:\n","            self.register_parameter(\"alpha_param\", None)\n","        else:\n","            self.alpha_param = self.init_scaling_parameter(alpha_init, in_features, device=device, dtype=dtype)\n","            self.register_parameter(\"alpha_param\", self.alpha_param)\n","\n","        if gamma_init is None:\n","            self.register_parameter(\"gamma_param\", None)\n","        else:\n","            self.gamma_param = self.init_scaling_parameter(gamma_init, out_features, device=device, dtype=dtype)\n","            self.register_parameter(\"gamma_param\", self.gamma_param)\n","\n","        if bias:\n","            self.bias_param = nn.Parameter(torch.zeros(ensemble_size, out_features, device=device, dtype=dtype))\n","            self.register_parameter(\"bias_param\", self.bias_param)\n","        else:\n","            self.register_parameter(\"bias_param\", None)\n","\n","    def init_scaling_parameter(self, init_value: float, num_features: int, device=None, dtype=None):\n","        param = torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype)\n","        if init_value < 0:\n","            param.normal_(mean=1, std=-init_value)\n","        else:\n","            random_sign_(param, prob=init_value, value=1.0)\n","        return nn.Parameter(param)\n","\n","    def expand_param(self, x: torch.Tensor, param: torch.Tensor):\n","        num_repeats = x.size(0) // self.ensemble_size\n","        expanded_param = torch.repeat_interleave(param, num_repeats, dim=0)\n","        extra_dims = len(x.shape) - len(expanded_param.shape)\n","        for _ in range(extra_dims):\n","            expanded_param = expanded_param.unsqueeze(-1)\n","        return expanded_param\n","\n","\n","class BELinear(nn.Linear, BatchEnsembleMixin):\n","    def __init__(\n","            self,\n","            in_features: int,\n","            out_features: int,\n","            bias: bool = True,\n","            ensemble_size: int = 1,\n","            alpha_init: float | None = None,\n","            gamma_init: float | None = None,\n","            device=None,\n","            dtype=None,\n","    ):\n","        super().__init__(in_features, out_features, bias=False, device=device, dtype=dtype)\n","        self.init_ensemble(in_features, out_features, ensemble_size, alpha_init, gamma_init, bias)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        if self.alpha_init is not None:\n","            input = input * self.expand_param(input, self.alpha_param)\n","        x = F.linear(input, self.weight)\n","        if self.gamma_init is not None:\n","            x = x * self.expand_param(x, self.gamma_param)\n","        if self.bias_param is not None:\n","            x = x + self.expand_param(x, self.bias_param)\n","        return x\n","\n","\n","class Conv2d(nn.Conv2d, BatchEnsembleMixin):\n","    def __init__(\n","            self,\n","            in_channels: int,\n","            out_channels: int,\n","            kernel_size,\n","            stride=1,\n","            padding=0,\n","            dilation=1,\n","            groups=1,\n","            bias: bool = True,\n","            padding_mode='zeros',\n","            ensemble_size: int = 1,\n","            alpha_init: float | None = None,\n","            gamma_init: float | None = None,\n","            device=None,\n","            dtype=None,\n","    ):\n","        super().__init__(\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            dilation=dilation,\n","            groups=groups,\n","            bias=False,  # Bias is managed separately\n","            padding_mode=padding_mode,\n","            device=device,\n","            dtype=dtype\n","        )\n","        self.init_ensemble(\n","            in_features=in_channels,\n","            out_features=out_channels,\n","            ensemble_size=ensemble_size,\n","            alpha_init=alpha_init,\n","            gamma_init=gamma_init,\n","            bias=bias,\n","            device=device,\n","            dtype=dtype\n","        )\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        if self.alpha_init is not None:\n","            input = input * self.expand_param(input, self.alpha_param)\n","        x = self._conv_forward(input, self.weight, None)\n","        if self.gamma_init is not None:\n","            x = x * self.expand_param(x, self.gamma_param)\n","        if self.bias_param is not None:\n","            x = x + self.expand_param(x, self.bias_param)\n","        return x\n","\n","\n","class BatchNorm2d(nn.BatchNorm2d):\n","    def __init__(\n","            self,\n","            num_features: int,\n","            eps=1e-5,\n","            momentum=0.1,\n","            affine=False,  # We will manage affine parameters ourselves\n","            track_running_stats=True,\n","            device=None,\n","            dtype=None,\n","            ensemble_size: int = 1,\n","    ):\n","        super().__init__(\n","            num_features,\n","            eps=eps,\n","            momentum=momentum,\n","            affine=affine,\n","            track_running_stats=track_running_stats,\n","            device=device,\n","            dtype=dtype\n","        )\n","        self.ensemble_size = ensemble_size\n","        # Correct tensor initialization\n","        self.weight_be = nn.Parameter(torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype))\n","        self.bias_be = nn.Parameter(torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype))\n","        self.reset_be_parameters()\n","\n","    def reset_be_parameters(self):\n","        nn.init.ones_(self.weight_be)\n","        nn.init.zeros_(self.bias_be)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        # Use the base class's forward method\n","        x = super().forward(input)\n","        num_repeats = x.size(0) // self.ensemble_size\n","        weight = torch.repeat_interleave(self.weight_be, num_repeats, dim=0).unsqueeze(2).unsqueeze(3)\n","        bias = torch.repeat_interleave(self.bias_be, num_repeats, dim=0).unsqueeze(2).unsqueeze(3)\n","        x = x * weight + bias\n","        return x\n","\n","\n","# StandardScaler\n","class StandardScaler:\n","    \"\"\"\n","    Based on https://gist.github.com/farahmand-m/8a416f33a27d73a149f92ce4708beb40\n","    Extended with inverse_transform\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            mean: torch.Tensor | None = None,\n","            std: torch.Tensor | None = None,\n","            epsilon: float = 1e-8,\n","    ):\n","        self.mean = mean\n","        self.std = std\n","        self.epsilon = epsilon\n","\n","    def fit(self, data: torch.Tensor):\n","        reduction_axes = list(range(data.dim() - 1))\n","        self.mean = torch.mean(data, dim=reduction_axes)\n","        self.std = torch.std(data, dim=reduction_axes)\n","        return self\n","\n","    def transform(self, data: torch.Tensor) -> torch.Tensor:\n","        if self.mean is None or self.std is None:\n","            raise RuntimeError(\"Call fit before calling transform.\")\n","        scaled_mean = (data - self.mean) / (self.std + self.epsilon)\n","        return scaled_mean\n","\n","    def inverse_transform(self, scaled_data: torch.Tensor) -> torch.Tensor:\n","        if self.mean is None or self.std is None:\n","            raise RuntimeError(\"Call fit before calling inverse_transform.\")\n","        unscaled_mean = scaled_data * (self.std + self.epsilon) + self.mean\n","        return unscaled_mean\n","\n","    def fit_transform(self, data: torch.Tensor) -> torch.Tensor:\n","        self.fit(data)\n","        return self.transform(data)\n","\n","    def to(self, target_device: torch.device):\n","        if self.mean is not None and self.std is not None:\n","            self.mean = self.mean.to(target_device)\n","            self.std = self.std.to(target_device)\n","        return self\n","\n","\n","# CNN Models\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.num_classes = 10\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Changed input channels to 3\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.fc = nn.Linear(64 * 8 * 8, 10)  # Adjusted input size\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))  # [batch_size, 32, 32, 32]\n","        x = self.pool(x)  # [batch_size, 32, 16, 16]\n","        x = self.relu(self.bn2(self.conv2(x)))  # [batch_size, 64, 16, 16]\n","        x = self.pool(x)  # [batch_size, 64, 8, 8]\n","        x = x.view(x.size(0), -1)  # [batch_size, 64*8*8]\n","        x = self.fc(x)  # [batch_size, 10]\n","        return x\n","\n","\n","class SharedParametersCNN(nn.Module):\n","    def __init__(self, num_heads=4):\n","        super(SharedParametersCNN, self).__init__()\n","        self.num_heads = num_heads\n","        self.num_classes = 10\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Changed input channels to 3\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.shared_fc = nn.Linear(64 * 8 * 8, 128)  # Adjusted input size\n","        # Multiple heads for classification\n","        self.heads = nn.ModuleList([nn.Linear(128, 10) for _ in range(num_heads)])\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for idx, head in enumerate(self.heads):\n","            torch.manual_seed(torch.seed() + idx)\n","            nn.init.kaiming_normal_(head.weight, mode='fan_out', nonlinearity='relu')\n","            if head.bias is not None:\n","                nn.init.zeros_(head.bias)\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))  # [batch_size, 32, 32, 32]\n","        x = self.pool(x)  # [batch_size, 32, 16, 16]\n","        x = self.relu(self.bn2(self.conv2(x)))  # [batch_size, 64, 16, 16]\n","        x = self.pool(x)  # [batch_size, 64, 8, 8]\n","        x = x.view(x.size(0), -1)  # [batch_size, 64*8*8]\n","        x = self.shared_fc(x)  # [batch_size, 128]\n","        # Collect outputs from all heads\n","        outputs = [head(x) for head in self.heads]\n","        outputs = torch.stack(outputs)  # Shape: [num_heads, batch_size, num_classes]\n","        # Average the outputs over heads\n","        x = outputs.mean(dim=0)  # Shape: [batch_size, num_classes]\n","        return x\n","\n","\n","class BatchEnsembleCNN(nn.Module):\n","    def __init__(self, ensemble_size=4, alpha_init=0.5, gamma_init=0.5):\n","        super(BatchEnsembleCNN, self).__init__()\n","        self.num_classes = 10\n","        self.ensemble_size = ensemble_size\n","        self.conv1 = Conv2d(3, 32, kernel_size=3, padding=1,  # Changed input channels to 3\n","                            ensemble_size=ensemble_size,\n","                            alpha_init=alpha_init,\n","                            gamma_init=gamma_init)\n","        self.bn1 = BatchNorm2d(32, ensemble_size=ensemble_size)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2)\n","        self.conv2 = Conv2d(32, 64, kernel_size=3, padding=1,\n","                            ensemble_size=ensemble_size,\n","                            alpha_init=alpha_init,\n","                            gamma_init=gamma_init)\n","        self.bn2 = BatchNorm2d(64, ensemble_size=ensemble_size)\n","        self.fc = BELinear(64 * 8 * 8, 10,  # Adjusted input size\n","                           ensemble_size=ensemble_size,\n","                           alpha_init=alpha_init,\n","                           gamma_init=gamma_init)\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, (Conv2d, BELinear)):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                # Initialize alpha_param and gamma_param differently for each ensemble member\n","                if m.alpha_param is not None:\n","                    for idx in range(m.ensemble_size):\n","                        torch.manual_seed(torch.seed() + idx)\n","                        random_sign_(m.alpha_param[idx], prob=m.alpha_init, value=1.0)\n","                if m.gamma_param is not None:\n","                    for idx in range(m.ensemble_size):\n","                        torch.manual_seed(torch.seed() + idx + 100)\n","                        random_sign_(m.gamma_param[idx], prob=m.gamma_init, value=1.0)\n","            elif isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.ones_(m.weight_be)\n","                nn.init.zeros_(m.bias_be)\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))  # [batch_size * ensemble_size, 32, 32, 32]\n","        x = self.pool(x)  # [batch_size * ensemble_size, 32, 16, 16]\n","        x = self.relu(self.bn2(self.conv2(x)))  # [batch_size * ensemble_size, 64, 16, 16]\n","        x = self.pool(x)  # [batch_size * ensemble_size, 64, 8, 8]\n","        x = x.view(x.size(0), -1)  # [batch_size * ensemble_size, 64*8*8]\n","        x = self.fc(x)  # [batch_size * ensemble_size, 10]\n","        return x\n","\n","\n","class SharedParametersBatchEnsembleCNN(nn.Module):\n","    def __init__(self, ensemble_size=4, alpha_init=0.5, gamma_init=0.5):\n","        super(SharedParametersBatchEnsembleCNN, self).__init__()\n","        self.ensemble_size = ensemble_size\n","        self.num_classes = 10\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # Shared BatchEnsemble convolutional layers\n","        self.conv1 = Conv2d(\n","            3, 32, kernel_size=3, padding=1,  # Changed input channels to 3\n","            ensemble_size=ensemble_size,\n","            alpha_init=alpha_init,\n","            gamma_init=gamma_init\n","        )\n","        self.bn1 = BatchNorm2d(32, ensemble_size=ensemble_size)\n","        self.conv2 = Conv2d(\n","            32, 64, kernel_size=3, padding=1,\n","            ensemble_size=ensemble_size,\n","            alpha_init=alpha_init,\n","            gamma_init=gamma_init\n","        )\n","        self.bn2 = BatchNorm2d(64, ensemble_size=ensemble_size)\n","\n","        # Shared fully connected layer\n","        self.shared_fc = BELinear(\n","            64 * 8 * 8, 128,  # Adjusted input size\n","            ensemble_size=ensemble_size,\n","            alpha_init=alpha_init,\n","            gamma_init=gamma_init\n","        )\n","\n","        # Multiple heads for classification\n","        self.heads = nn.ModuleList([\n","            nn.Linear(128, self.num_classes) for _ in range(ensemble_size)\n","        ])\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, (Conv2d, BELinear)):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                # Initialize alpha_param and gamma_param differently for each ensemble member\n","                if m.alpha_param is not None:\n","                    for idx in range(m.ensemble_size):\n","                        torch.manual_seed(torch.seed() + idx)\n","                        random_sign_(m.alpha_param[idx], prob=m.alpha_init, value=1.0)\n","                if m.gamma_param is not None:\n","                    for idx in range(m.ensemble_size):\n","                        torch.manual_seed(torch.seed() + idx + 100)\n","                        random_sign_(m.gamma_param[idx], prob=m.gamma_init, value=1.0)\n","            elif isinstance(m, nn.Linear):\n","                for idx, head in enumerate(self.heads):\n","                    torch.manual_seed(torch.seed() + idx)\n","                    nn.init.kaiming_normal_(head.weight, mode='fan_out', nonlinearity='relu')\n","                    if head.bias is not None:\n","                        nn.init.zeros_(head.bias)\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.ones_(m.weight_be)\n","                nn.init.zeros_(m.bias_be)\n","\n","    def forward(self, x):\n","        # Shared convolutional layers\n","        x = self.relu(self.bn1(self.conv1(x)))  # Shape: [batch_size * ensemble_size, 32, 32, 32]\n","        x = self.pool(x)  # Shape: [batch_size * ensemble_size, 32, 16, 16]\n","        x = self.relu(self.bn2(self.conv2(x)))  # Shape: [batch_size * ensemble_size, 64, 16, 16]\n","        x = self.pool(x)  # Shape: [batch_size * ensemble_size, 64, 8, 8]\n","        x = x.view(x.size(0), -1)  # Shape: [batch_size * ensemble_size, 64*8*8]\n","\n","        # Shared fully connected layer\n","        x = self.shared_fc(x)  # Shape: [batch_size * ensemble_size, 128]\n","\n","        # Reshape x to [ensemble_size, batch_size, 128]\n","        batch_size = x.size(0) // self.ensemble_size\n","        x = x.view(self.ensemble_size, batch_size, -1)\n","\n","        # Apply each head and collect outputs\n","        outputs = []\n","        for i, head in enumerate(self.heads):\n","            out = head(x[i])  # x[i] is [batch_size, 128]\n","            outputs.append(out)\n","\n","        # Stack outputs: Shape [ensemble_size, batch_size, num_classes]\n","        outputs = torch.stack(outputs)  # Shape: [ensemble_size, batch_size, num_classes]\n","\n","        # Average over ensemble members\n","        outputs = outputs.mean(dim=0)  # Shape: [batch_size, num_classes]\n","\n","        return outputs\n","\n","\n","class ComplexCNN(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ComplexCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            # Conv1\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # 32x32x3 -> 32x32x64\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 32x32x64 -> 16x16x64\n","\n","            # Conv2\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # 16x16x64 -> 16x16x128\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16x128 -> 8x8x128\n","\n","            # Conv3\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # 8x8x128 -> 8x8x256\n","            nn.ReLU(inplace=True),\n","\n","            # Conv4\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 8x8x256 -> 8x8x256\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 8x8x256 -> 4x4x256\n","\n","            # Conv5\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # 4x4x256 -> 4x4x512\n","            nn.ReLU(inplace=True),\n","\n","            # Conv6\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 4x4x512 -> 4x4x512\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 4x4x512 -> 2x2x512\n","\n","            # Conv7\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 2x2x512 -> 2x2x512\n","            nn.ReLU(inplace=True),\n","\n","            # Conv8\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 2x2x512 -> 2x2x512\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 2x2x512 -> 1x1x512\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 1 * 1, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)  # Flatten\n","        x = self.classifier(x)\n","        return x\n","\n","\n","class BatchEnsembleComplexCNN(nn.Module):\n","    def __init__(self, num_classes=10, ensemble_size=4, alpha_init=0.5, gamma_init=0.5):\n","        super(BatchEnsembleComplexCNN, self).__init__()\n","        self.num_classes = num_classes\n","        self.ensemble_size = ensemble_size\n","\n","        # Define BatchEnsemble convolutional layers\n","        self.features = nn.Sequential(\n","            # Conv1\n","            Conv2d(3, 64, kernel_size=3, padding=1,  # Input: 32x32x3 -> Output: 32x32x64\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(64, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x16x64\n","\n","            # Conv2\n","            Conv2d(64, 128, kernel_size=3, padding=1,  # Output: 16x16x128\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(128, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 8x8x128\n","\n","            # Conv3\n","            Conv2d(128, 256, kernel_size=3, padding=1,  # Output: 8x8x256\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(256, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","\n","            # Conv4\n","            Conv2d(256, 256, kernel_size=3, padding=1,  # Output: 8x8x256\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(256, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 4x4x256\n","\n","            # Conv5\n","            Conv2d(256, 512, kernel_size=3, padding=1,  # Output: 4x4x512\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(512, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","\n","            # Conv6\n","            Conv2d(512, 512, kernel_size=3, padding=1,  # Output: 4x4x512\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(512, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 2x2x512\n","\n","            # Conv7\n","            Conv2d(512, 512, kernel_size=3, padding=1,  # Output: 2x2x512\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(512, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","\n","            # Conv8\n","            Conv2d(512, 512, kernel_size=3, padding=1,  # Output: 2x2x512\n","                   ensemble_size=ensemble_size,\n","                   alpha_init=alpha_init,\n","                   gamma_init=gamma_init),\n","            BatchNorm2d(512, ensemble_size=ensemble_size),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 1x1x512\n","        )\n","\n","        # Define BatchEnsemble classifier\n","        self.classifier = nn.Sequential(\n","            BELinear(512 * 1 * 1, 512,\n","                     ensemble_size=ensemble_size,\n","                     alpha_init=alpha_init,\n","                     gamma_init=gamma_init),\n","            nn.ReLU(inplace=True),\n","            BELinear(512, num_classes,\n","                     ensemble_size=ensemble_size,\n","                     alpha_init=alpha_init,\n","                     gamma_init=gamma_init)\n","        )\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, (Conv2d, BELinear)):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                # Initialize alpha_param and gamma_param differently for each ensemble member\n","                if m.alpha_param is not None:\n","                    for idx in range(m.ensemble_size):\n","                        torch.manual_seed(torch.seed() + idx)\n","                        random_sign_(m.alpha_param[idx], prob=m.alpha_init, value=1.0)\n","                if m.gamma_param is not None:\n","                    for idx in range(m.ensemble_size):\n","                        torch.manual_seed(torch.seed() + idx + 100)\n","                        random_sign_(m.gamma_param[idx], prob=m.gamma_init, value=1.0)\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.ones_(m.weight_be)\n","                nn.init.zeros_(m.bias_be)\n","\n","    def forward(self, x):\n","        # Forward pass through convolutional layers\n","        x = self.features(x)  # Shape: [batch_size * ensemble_size, 1, 1, 512]\n","\n","        # Flatten\n","        x = x.view(x.size(0), -1)  # Shape: [batch_size * ensemble_size, 512]\n","\n","        # Forward pass through classifier\n","        x = self.classifier(x)  # Shape: [batch_size * ensemble_size, num_classes]\n","\n","        return x\n"]},{"cell_type":"code","source":["# Training and visualization functions\n","\n","def train_cifar_model(\n","        model,\n","        train_loader,\n","        test_loader,\n","        model_type=\"simple\",\n","        ensemble_size=4,\n","        num_epochs=15,\n","        lr=0.0002,\n","        device=torch.device(\"cpu\"),\n","        optimizer_type=\"adam\"\n","):\n","    model.to(device)  # Move the model to the GPU if available\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Set the optimizer\n","    if optimizer_type == \"adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","    elif optimizer_type == \"sgd\":\n","        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","    elif optimizer_type == \"ivon\":\n","        optimizer = ivon.IVON(model.parameters(), lr=lr, ess=len(train_loader))\n","    else:\n","        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")\n","\n","\n","    # Metrics storage\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    if optimizer_type == \"ivon\":\n","\n","        for epoch in range(num_epochs):\n","            # Training phase\n","            model.train()\n","            epoch_loss = 0.0\n","            correct_train = 0\n","            total_train = 0\n","\n","            for images, labels in train_loader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","\n","                with optimizer.sampled_params(train=True):\n","                    optimizer.zero_grad()\n","                    if model_type in [\"batchensemble\", \"batchensemble_complex\"]:\n","                        images = images.repeat(ensemble_size, 1, 1, 1)\n","                        labels = labels.repeat(ensemble_size)\n","                    elif model_type == \"shared_batchensemble\":\n","                        images = images.repeat(ensemble_size, 1, 1, 1)\n","\n","                    outputs = model(images)\n","                    if model_type == \"shared_batchensemble\":\n","                        outputs = outputs.view(ensemble_size, -1, model.num_classes).mean(dim=0)\n","\n","                    loss = criterion(outputs, labels)\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total_train += labels.size(0)\n","                correct_train += (predicted == labels).sum().item()\n","\n","            train_losses.append(epoch_loss / len(train_loader))\n","            train_accuracy = 100 * correct_train / total_train\n","            train_accuracies.append(train_accuracy)\n","\n","            # Validation phase\n","            model.eval()\n","            correct_val = 0\n","            total_val = 0\n","            val_loss = 0.0\n","\n","            with torch.no_grad():\n","                for images, labels in test_loader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","\n","                    if model_type in [\"batchensemble\", \"batchensemble_complex\"]:\n","                        images = images.repeat(ensemble_size, 1, 1, 1)\n","                        outputs = model(images)\n","                        outputs = outputs.view(ensemble_size, -1, model.num_classes).mean(dim=0)\n","                    elif model_type == \"shared_batchensemble\":\n","                        images = images.repeat(ensemble_size, 1, 1, 1)\n","                        outputs = model(images)\n","                    else:\n","                        outputs = model(images)\n","\n","                    loss = criterion(outputs, labels)\n","                    val_loss += loss.item()\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total_val += labels.size(0)\n","                    correct_val += (predicted == labels).sum().item()\n","\n","            val_losses.append(val_loss / len(test_loader))\n","            val_accuracy = 100 * correct_val / total_val\n","            val_accuracies.append(val_accuracy)\n","\n","            print(f'Epoch [{epoch + 1}/{num_epochs}]')\n","            print(f'Training Loss: {train_losses[-1]:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n","            print(f'Validation Loss: {val_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n","\n","    else:\n","        for epoch in range(num_epochs):\n","            # Training phase\n","            model.train()\n","            epoch_loss = 0.0\n","            correct_train = 0\n","            total_train = 0\n","\n","            for images, labels in train_loader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                optimizer.zero_grad()\n","\n","                if model_type in [\"batchensemble\", \"batchensemble_complex\"]:\n","                    images = images.repeat(ensemble_size, 1, 1, 1)\n","                    labels = labels.repeat(ensemble_size)\n","                elif model_type == \"shared_batchensemble\":\n","                    images = images.repeat(ensemble_size, 1, 1, 1)\n","\n","                outputs = model(images)\n","                if model_type == \"shared_batchensemble\":\n","                    outputs = outputs.view(ensemble_size, -1, model.num_classes).mean(dim=0)\n","\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total_train += labels.size(0)\n","                correct_train += (predicted == labels).sum().item()\n","\n","            train_losses.append(epoch_loss / len(train_loader))\n","            train_accuracy = 100 * correct_train / total_train\n","            train_accuracies.append(train_accuracy)\n","\n","            # Validation phase\n","            model.eval()\n","            correct_val = 0\n","            total_val = 0\n","            val_loss = 0.0\n","\n","            with torch.no_grad():\n","                for images, labels in test_loader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","\n","                    if model_type in [\"batchensemble\", \"batchensemble_complex\"]:\n","                        images = images.repeat(ensemble_size, 1, 1, 1)\n","                        outputs = model(images)\n","                        outputs = outputs.view(ensemble_size, -1, model.num_classes).mean(dim=0)\n","                    elif model_type == \"shared_batchensemble\":\n","                        images = images.repeat(ensemble_size, 1, 1, 1)\n","                        outputs = model(images)\n","                    else:\n","                        outputs = model(images)\n","\n","                    loss = criterion(outputs, labels)\n","                    val_loss += loss.item()\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total_val += labels.size(0)\n","                    correct_val += (predicted == labels).sum().item()\n","\n","            val_losses.append(val_loss / len(test_loader))\n","            val_accuracy = 100 * correct_val / total_val\n","            val_accuracies.append(val_accuracy)\n","\n","            print(f'Epoch [{epoch + 1}/{num_epochs}]')\n","            print(f'Training Loss: {train_losses[-1]:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n","            print(f'Validation Loss: {val_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies\n","\n","\n","# Main function\n","def train_with_params(\n","        seed: int = typer.Option(42, help=\"Size of the seed\"),\n","        ensemble_size: int = typer.Option(4, help=\"Size of the ensemble\"),\n","        alpha_init: float = typer.Option(0.5, help=\"Initial value for Alpha in BatchEnsemble\"),\n","        gamma_init: float = typer.Option(0.5, help=\"Initial value for Gamma in BatchEnsemble\"),\n","        num_epochs: int = typer.Option(15, help=\"Number of training epochs\"),\n","        batch_size: int = typer.Option(256, help=\"Batch size for training\"),\n","        lr: float = typer.Option(0.0002, help=\"Learning rate for the optimizer\"),\n","        device: str = typer.Option(\"cpu\", help=\"Device to use, 'cuda' for GPU or 'cpu' for CPU\"),\n","        optimizer_type = typer.Option(\"adam\", help=\"Type of Optimizer\"),\n","        mode: str = typer.Option(\"parameter\", help=\"Mode if parameter or optimizer testing\")\n","):\n","    # Removed the global random seed to allow different initializations\n","    # torch.manual_seed(seed)\n","    num_heads = ensemble_size  # For consistency\n","\n","    # Ensure that batch_size is divisible by ensemble_size\n","    if batch_size % ensemble_size != 0:\n","        raise ValueError(\"Batch size must be divisible by ensemble size.\")\n","\n","    # Define transformations for CIFAR-10\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 mean\n","            std=[0.2023, 0.1994, 0.2010]  # CIFAR-10 std\n","        ),\n","    ])\n","\n","    # Load CIFAR-10 dataset\n","    train_dataset = datasets.CIFAR10(root='./data', train=True,\n","                                     download=True, transform=transform)\n","    test_dataset = datasets.CIFAR10(root='./data', train=False,\n","                                    download=True, transform=transform)\n","\n","    # Data loaders\n","    train_loader = torch.utils.data.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True\n","    )\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        dataset=test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False\n","    )\n","\n","    # Define model types, including 'complex' and 'batchensemble_complex' -> change for specific cases\n","    model_types = [\n","        # \"simple\",\n","        \"batchensemble\",\n","        # \"sharedparameters\",\n","        # \"shared_batchensemble\",\n","        # \"complex\",\n","        # \"batchensemble_complex\"\n","    ]\n","\n","    # Initialize dictionaries to store CNN models and their metrics\n","    cnn_models = {}\n","    cnn_train_losses = {}\n","    cnn_test_accuracies = {}\n","    cnn_val_losses = {}\n","    cnn_train_acc = {}\n","    cnn_val_acc = {}\n","\n","    # for the differences\n","    results = []\n","\n","    # Train and evaluate CNN models\n","    for model_type in model_types:\n","        if model_type == \"simple\":\n","            model = SimpleCNN().to(device)\n","            ensemble_size_cnn = 1\n","        elif model_type == \"batchensemble\":\n","            model = BatchEnsembleCNN(\n","                ensemble_size=ensemble_size,\n","                alpha_init=alpha_init,\n","                gamma_init=gamma_init\n","            ).to(device)\n","            ensemble_size_cnn = ensemble_size\n","        elif model_type == \"sharedparameters\":\n","            model = SharedParametersCNN(\n","                num_heads=num_heads\n","            ).to(device)\n","            ensemble_size_cnn = 1  # Shared parameters handle multiple heads internally\n","        elif model_type == \"shared_batchensemble\":\n","            model = SharedParametersBatchEnsembleCNN(\n","                ensemble_size=ensemble_size,\n","                alpha_init=alpha_init,\n","                gamma_init=gamma_init\n","            ).to(device)\n","            ensemble_size_cnn = ensemble_size\n","        elif model_type == \"complex\":\n","            model = ComplexCNN().to(device)\n","            ensemble_size_cnn = 1  # No ensemble in this model\n","        elif model_type == \"batchensemble_complex\":\n","            model = BatchEnsembleComplexCNN(\n","                ensemble_size=ensemble_size,\n","                alpha_init=alpha_init,\n","                gamma_init=gamma_init\n","            ).to(device)\n","            ensemble_size_cnn = ensemble_size\n","        else:\n","            raise ValueError(\"Invalid model_type.\")\n","\n","        if mode == \"parameter\":\n","            cnn_models[model_type] = model\n","            print(f\"Training CNN model: {model_type}\")\n","            tr_losses, val_losses, train_accuracies, te_accuracies = train_cifar_model(\n","                model,\n","                train_loader,\n","                test_loader,\n","                model_type=model_type,\n","                ensemble_size=ensemble_size_cnn,\n","                num_epochs=num_epochs,\n","                lr=lr,\n","                device=device,  # Ensure device is passed\n","            )\n","            cnn_train_losses[model_type] = tr_losses\n","            cnn_test_accuracies[model_type] = te_accuracies\n","            result = {\n","                \"seed\": seed,\n","                \"alpha_init\": alpha_init,\n","                \"gamma_init\": gamma_init,\n","                \"ensemble_size\": ensemble_size,\n","                \"num_epochs\": num_epochs,\n","                \"batch_size\": batch_size,\n","                \"lr\": lr,\n","                \"model_type\": model_type,\n","                \"final_test_accuracy\": te_accuracies[-1],\n","                \"avg_test_accuracy\": sum(te_accuracies) / len(te_accuracies),\n","                \"final_losses\": tr_losses[-1],\n","                \"avg_losses\": sum(tr_losses) / len(tr_losses)\n","            }\n","            results.append(result)\n","        elif mode == \"optimizer\":\n","            cnn_models[model_type] = model\n","            print(f\"Training CNN model: {model_type}\")\n","            train_losses, val_losses, train_accuracies, val_accuracies = train_cifar_model(\n","                model,\n","                train_loader,\n","                test_loader,\n","                model_type=model_type,\n","                ensemble_size=ensemble_size_cnn,\n","                num_epochs=num_epochs,\n","                lr=lr,\n","                device=device,  # Ensure device is passed\n","            )\n","\n","            cnn_train_losses[model_type] = train_losses\n","            cnn_val_losses[model_type] = val_losses\n","            cnn_train_acc[model_type] = train_accuracies\n","            cnn_val_acc[model_type] = val_accuracies\n","\n","            result = {\n","                \"ensemble_size\": ensemble_size,\n","                \"num_epochs\": num_epochs,\n","                \"batch_size\": batch_size,\n","                \"lr\": lr,\n","                \"model_type\": model_type,\n","                \"train_loss\": cnn_train_losses[model_type],\n","                \"val_loss\": cnn_val_losses[model_type],\n","                \"train_acc\": cnn_train_acc[model_type],\n","                \"val_acc\": cnn_val_acc[model_type]\n","            }\n","            results.append(result)\n","\n","    return results\n","\n","\n","def save_results_to_csv(results, file_path):\n","    # Check if file exists\n","    file_exists = os.path.isfile(file_path)\n","\n","    # Get the headers from the first dictionary in results\n","    headers = results[0].keys() if results else []\n","\n","    # Open the file in append mode\n","    with open(file_path, mode='a', newline='', encoding='utf-8') as file:\n","        writer = csv.DictWriter(file, fieldnames=headers)\n","\n","        # Write the header only if the file does not exist\n","        if not file_exists:\n","            writer.writeheader()\n","\n","        # Write each result as a row\n","        for result in results:\n","            writer.writerow(result)\n","\n","    if file_exists:\n","        print(f\"Appended results to {file_path}\")\n","    else:\n","        print(f\"Created new results file: {file_path}\")\n"],"metadata":{"id":"Hl7aJCTxq4rz","executionInfo":{"status":"ok","timestamp":1734708849077,"user_tz":-60,"elapsed":8,"user":{"displayName":"Justine","userId":"10003038839395646781"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Enable batchensemble or complexensemble in training"],"metadata":{"id":"ovxngpWG0xeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test paramter alpha, gamma and ensemble size\n","\n","\"\"\"Defines a CLI to test multiple parameter combinations.\"\"\"\n","alpha_list = [0.2, 0.8] #[-1, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1]\n","gamma_list =  [0.2, 0.8] #[-1, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1]\n","ensemble_size_list = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n","lr_list = [0.02, 0.002, 0.0002]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","output_folder = \"data\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","alpha_gamma_file = os.path.join(output_folder, \"alpha_gamma_results-loss.csv\")\n","ensemble_size_file = os.path.join(output_folder, \"ensemble_size_results-loss.csv\")\n","\n","\n","# test different alpha and gamma values\n","for alpha in alpha_list:\n","    for gamma in gamma_list:\n","        print(f\"Next calculation with differences in alpha and gamma: alpha={alpha} and gamma={gamma}\")\n","        results = train_with_params(seed=42, ensemble_size=2, alpha_init=alpha, gamma_init=gamma, num_epochs=15, batch_size=256, lr=0.0002, device=device, mode=\"parameter\")\n","        print(results)\n","        #save_results_to_csv(results, alpha_gamma_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4gfFILmvYF9","executionInfo":{"status":"ok","timestamp":1734712462963,"user_tz":-60,"elapsed":1297798,"user":{"displayName":"Justine","userId":"10003038839395646781"}},"outputId":"43478904-94df-4911-cac7-d82f146196d1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Next calculation with differences in alpha and gamma: alpha=0.2 and gamma=0.2\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 13.7760, Training Accuracy: 19.31%\n","Validation Loss: 5.4074, Validation Accuracy: 27.48%\n","Epoch [2/15]\n","Training Loss: 5.5415, Training Accuracy: 24.95%\n","Validation Loss: 3.2126, Validation Accuracy: 30.59%\n","Epoch [3/15]\n","Training Loss: 3.6930, Training Accuracy: 26.92%\n","Validation Loss: 2.5003, Validation Accuracy: 33.11%\n","Epoch [4/15]\n","Training Loss: 2.9097, Training Accuracy: 28.61%\n","Validation Loss: 2.1256, Validation Accuracy: 34.62%\n","Epoch [5/15]\n","Training Loss: 2.5171, Training Accuracy: 30.18%\n","Validation Loss: 1.9595, Validation Accuracy: 35.57%\n","Epoch [6/15]\n","Training Loss: 2.2787, Training Accuracy: 31.68%\n","Validation Loss: 1.8480, Validation Accuracy: 37.60%\n","Epoch [7/15]\n","Training Loss: 2.1264, Training Accuracy: 33.13%\n","Validation Loss: 1.7827, Validation Accuracy: 38.56%\n","Epoch [8/15]\n","Training Loss: 2.0191, Training Accuracy: 34.45%\n","Validation Loss: 1.7135, Validation Accuracy: 40.24%\n","Epoch [9/15]\n","Training Loss: 1.9408, Training Accuracy: 35.65%\n","Validation Loss: 1.6852, Validation Accuracy: 41.08%\n","Epoch [10/15]\n","Training Loss: 1.8720, Training Accuracy: 37.07%\n","Validation Loss: 1.6385, Validation Accuracy: 41.77%\n","Epoch [11/15]\n","Training Loss: 1.8196, Training Accuracy: 38.06%\n","Validation Loss: 1.6297, Validation Accuracy: 42.36%\n","Epoch [12/15]\n","Training Loss: 1.7788, Training Accuracy: 39.09%\n","Validation Loss: 1.6022, Validation Accuracy: 42.55%\n","Epoch [13/15]\n","Training Loss: 1.7392, Training Accuracy: 40.01%\n","Validation Loss: 1.5648, Validation Accuracy: 44.60%\n","Epoch [14/15]\n","Training Loss: 1.7049, Training Accuracy: 40.99%\n","Validation Loss: 1.5387, Validation Accuracy: 45.02%\n","Epoch [15/15]\n","Training Loss: 1.6779, Training Accuracy: 41.77%\n","Validation Loss: 1.5098, Validation Accuracy: 46.42%\n","[{'seed': 42, 'alpha_init': 0.2, 'gamma_init': 0.2, 'ensemble_size': 2, 'num_epochs': 15, 'batch_size': 256, 'lr': 0.0002, 'model_type': 'batchensemble', 'final_test_accuracy': 46.42, 'avg_test_accuracy': 38.77133333333333, 'final_losses': 1.6778911558949217, 'avg_losses': 3.1596454980827517}]\n","Next calculation with differences in alpha and gamma: alpha=0.2 and gamma=0.8\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 13.1768, Training Accuracy: 19.43%\n","Validation Loss: 4.7720, Validation Accuracy: 29.45%\n","Epoch [2/15]\n","Training Loss: 5.2810, Training Accuracy: 25.39%\n","Validation Loss: 2.9953, Validation Accuracy: 31.72%\n","Epoch [3/15]\n","Training Loss: 3.5829, Training Accuracy: 27.98%\n","Validation Loss: 2.3829, Validation Accuracy: 34.40%\n","Epoch [4/15]\n","Training Loss: 2.8784, Training Accuracy: 30.05%\n","Validation Loss: 2.0668, Validation Accuracy: 36.86%\n","Epoch [5/15]\n","Training Loss: 2.4820, Training Accuracy: 31.71%\n","Validation Loss: 1.8793, Validation Accuracy: 38.18%\n","Epoch [6/15]\n","Training Loss: 2.2296, Training Accuracy: 33.27%\n","Validation Loss: 1.7596, Validation Accuracy: 39.92%\n","Epoch [7/15]\n","Training Loss: 2.0703, Training Accuracy: 34.87%\n","Validation Loss: 1.6838, Validation Accuracy: 41.85%\n","Epoch [8/15]\n","Training Loss: 1.9515, Training Accuracy: 36.23%\n","Validation Loss: 1.6370, Validation Accuracy: 43.40%\n","Epoch [9/15]\n","Training Loss: 1.8742, Training Accuracy: 37.36%\n","Validation Loss: 1.6106, Validation Accuracy: 43.76%\n","Epoch [10/15]\n","Training Loss: 1.8060, Training Accuracy: 38.70%\n","Validation Loss: 1.5611, Validation Accuracy: 44.60%\n","Epoch [11/15]\n","Training Loss: 1.7560, Training Accuracy: 39.81%\n","Validation Loss: 1.5394, Validation Accuracy: 45.61%\n","Epoch [12/15]\n","Training Loss: 1.7074, Training Accuracy: 41.11%\n","Validation Loss: 1.5138, Validation Accuracy: 46.67%\n","Epoch [13/15]\n","Training Loss: 1.6718, Training Accuracy: 41.93%\n","Validation Loss: 1.4885, Validation Accuracy: 47.93%\n","Epoch [14/15]\n","Training Loss: 1.6349, Training Accuracy: 43.07%\n","Validation Loss: 1.4679, Validation Accuracy: 48.32%\n","Epoch [15/15]\n","Training Loss: 1.6079, Training Accuracy: 43.92%\n","Validation Loss: 1.4745, Validation Accuracy: 48.58%\n","[{'seed': 42, 'alpha_init': 0.2, 'gamma_init': 0.8, 'ensemble_size': 2, 'num_epochs': 15, 'batch_size': 256, 'lr': 0.0002, 'model_type': 'batchensemble', 'final_test_accuracy': 48.58, 'avg_test_accuracy': 41.41666666666667, 'final_losses': 1.6079250598440364, 'avg_losses': 3.0473805871139574}]\n","Next calculation with differences in alpha and gamma: alpha=0.8 and gamma=0.2\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 13.2036, Training Accuracy: 19.56%\n","Validation Loss: 5.0378, Validation Accuracy: 28.16%\n","Epoch [2/15]\n","Training Loss: 5.4351, Training Accuracy: 25.63%\n","Validation Loss: 3.2752, Validation Accuracy: 31.90%\n","Epoch [3/15]\n","Training Loss: 3.8383, Training Accuracy: 27.84%\n","Validation Loss: 2.5770, Validation Accuracy: 34.01%\n","Epoch [4/15]\n","Training Loss: 3.0665, Training Accuracy: 29.79%\n","Validation Loss: 2.2184, Validation Accuracy: 35.21%\n","Epoch [5/15]\n","Training Loss: 2.6246, Training Accuracy: 31.54%\n","Validation Loss: 2.0290, Validation Accuracy: 36.83%\n","Epoch [6/15]\n","Training Loss: 2.3527, Training Accuracy: 33.01%\n","Validation Loss: 1.8817, Validation Accuracy: 38.48%\n","Epoch [7/15]\n","Training Loss: 2.1643, Training Accuracy: 34.52%\n","Validation Loss: 1.7843, Validation Accuracy: 39.48%\n","Epoch [8/15]\n","Training Loss: 2.0285, Training Accuracy: 35.84%\n","Validation Loss: 1.7177, Validation Accuracy: 40.63%\n","Epoch [9/15]\n","Training Loss: 1.9295, Training Accuracy: 37.31%\n","Validation Loss: 1.6755, Validation Accuracy: 41.64%\n","Epoch [10/15]\n","Training Loss: 1.8515, Training Accuracy: 38.29%\n","Validation Loss: 1.6212, Validation Accuracy: 43.01%\n","Epoch [11/15]\n","Training Loss: 1.7921, Training Accuracy: 39.22%\n","Validation Loss: 1.5914, Validation Accuracy: 44.08%\n","Epoch [12/15]\n","Training Loss: 1.7411, Training Accuracy: 40.53%\n","Validation Loss: 1.5741, Validation Accuracy: 44.24%\n","Epoch [13/15]\n","Training Loss: 1.7028, Training Accuracy: 41.33%\n","Validation Loss: 1.5466, Validation Accuracy: 45.01%\n","Epoch [14/15]\n","Training Loss: 1.6681, Training Accuracy: 42.20%\n","Validation Loss: 1.5201, Validation Accuracy: 46.29%\n","Epoch [15/15]\n","Training Loss: 1.6389, Training Accuracy: 42.94%\n","Validation Loss: 1.5069, Validation Accuracy: 46.90%\n","[{'seed': 42, 'alpha_init': 0.8, 'gamma_init': 0.2, 'ensemble_size': 2, 'num_epochs': 15, 'batch_size': 256, 'lr': 0.0002, 'model_type': 'batchensemble', 'final_test_accuracy': 46.9, 'avg_test_accuracy': 39.72466666666666, 'final_losses': 1.6388576760583995, 'avg_losses': 3.1358377301774056}]\n","Next calculation with differences in alpha and gamma: alpha=0.8 and gamma=0.8\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 12.6908, Training Accuracy: 18.65%\n","Validation Loss: 4.5648, Validation Accuracy: 28.78%\n","Epoch [2/15]\n","Training Loss: 4.8648, Training Accuracy: 25.68%\n","Validation Loss: 2.8660, Validation Accuracy: 32.08%\n","Epoch [3/15]\n","Training Loss: 3.3206, Training Accuracy: 28.38%\n","Validation Loss: 2.2842, Validation Accuracy: 34.81%\n","Epoch [4/15]\n","Training Loss: 2.6925, Training Accuracy: 30.34%\n","Validation Loss: 2.0196, Validation Accuracy: 36.30%\n","Epoch [5/15]\n","Training Loss: 2.3700, Training Accuracy: 32.07%\n","Validation Loss: 1.8701, Validation Accuracy: 37.96%\n","Epoch [6/15]\n","Training Loss: 2.1609, Training Accuracy: 33.62%\n","Validation Loss: 1.7813, Validation Accuracy: 39.56%\n","Epoch [7/15]\n","Training Loss: 2.0269, Training Accuracy: 35.26%\n","Validation Loss: 1.7174, Validation Accuracy: 41.19%\n","Epoch [8/15]\n","Training Loss: 1.9286, Training Accuracy: 36.50%\n","Validation Loss: 1.6613, Validation Accuracy: 42.28%\n","Epoch [9/15]\n","Training Loss: 1.8518, Training Accuracy: 38.01%\n","Validation Loss: 1.6363, Validation Accuracy: 42.54%\n","Epoch [10/15]\n","Training Loss: 1.7921, Training Accuracy: 39.10%\n","Validation Loss: 1.6049, Validation Accuracy: 43.66%\n","Epoch [11/15]\n","Training Loss: 1.7406, Training Accuracy: 40.33%\n","Validation Loss: 1.5646, Validation Accuracy: 44.86%\n","Epoch [12/15]\n","Training Loss: 1.7024, Training Accuracy: 41.24%\n","Validation Loss: 1.5542, Validation Accuracy: 45.07%\n","Epoch [13/15]\n","Training Loss: 1.6645, Training Accuracy: 42.22%\n","Validation Loss: 1.5157, Validation Accuracy: 46.26%\n","Epoch [14/15]\n","Training Loss: 1.6328, Training Accuracy: 43.23%\n","Validation Loss: 1.4899, Validation Accuracy: 47.48%\n","Epoch [15/15]\n","Training Loss: 1.6063, Training Accuracy: 43.83%\n","Validation Loss: 1.4739, Validation Accuracy: 48.16%\n","[{'seed': 42, 'alpha_init': 0.8, 'gamma_init': 0.8, 'ensemble_size': 2, 'num_epochs': 15, 'batch_size': 256, 'lr': 0.0002, 'model_type': 'batchensemble', 'final_test_accuracy': 48.16, 'avg_test_accuracy': 40.732666666666674, 'final_losses': 1.6063465707156124, 'avg_losses': 2.9363849494732976}]\n"]}]},{"cell_type":"code","source":["# Test paramter alpha, gamma and ensemble size\n","\n","\"\"\"Defines a CLI to test multiple parameter combinations.\"\"\"\n","alpha_list = [-1, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1]\n","gamma_list =  [-1, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1]\n","ensemble_size_list = [4, 8]\n","lr_list = [0.002]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","output_folder = \"data\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","alpha_gamma_file = os.path.join(output_folder, \"alpha_gamma_results-loss.csv\")\n","ensemble_size_file = os.path.join(output_folder, \"ensemble_size_results-loss.csv\")\n","\n","# test ensemble size\n","for lr in lr_list:\n","      for ensemble_size in ensemble_size_list:\n","            print(f\"Next calculation with differences in ensemble size: ensemble size = {ensemble_size}\")\n","            results = train_with_params(seed=42, ensemble_size=ensemble_size, alpha_init=0.5, gamma_init=0.5, num_epochs=15, batch_size=ensemble_size, lr=lr, device=device, mode=\"parameter\")\n","            print(results)\n","            #save_results_to_csv(results, ensemble_size_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPh7r62ZvbUf","executionInfo":{"status":"ok","timestamp":1734717776648,"user_tz":-60,"elapsed":1898099,"user":{"displayName":"Justine","userId":"10003038839395646781"}},"outputId":"a0c0b727-5709-4d7b-f2b2-3599dc158279"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Next calculation with differences in ensemble size: ensemble size = 4\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 1.8961, Training Accuracy: 42.80%\n","Validation Loss: 1.1947, Validation Accuracy: 57.33%\n","Epoch [2/15]\n","Training Loss: 1.1518, Training Accuracy: 59.76%\n","Validation Loss: 1.0395, Validation Accuracy: 64.08%\n","Epoch [3/15]\n","Training Loss: 1.0116, Training Accuracy: 64.83%\n","Validation Loss: 0.9156, Validation Accuracy: 68.16%\n","Epoch [4/15]\n","Training Loss: 0.9408, Training Accuracy: 67.36%\n","Validation Loss: 0.8789, Validation Accuracy: 70.29%\n","Epoch [5/15]\n","Training Loss: 0.8942, Training Accuracy: 69.12%\n","Validation Loss: 0.8400, Validation Accuracy: 71.70%\n","Epoch [6/15]\n","Training Loss: 0.8567, Training Accuracy: 70.39%\n","Validation Loss: 0.8266, Validation Accuracy: 71.65%\n","Epoch [7/15]\n","Training Loss: 0.8285, Training Accuracy: 71.33%\n","Validation Loss: 0.8030, Validation Accuracy: 72.73%\n","Epoch [8/15]\n","Training Loss: 0.8065, Training Accuracy: 72.03%\n","Validation Loss: 0.8053, Validation Accuracy: 72.79%\n","Epoch [9/15]\n","Training Loss: 0.7855, Training Accuracy: 72.67%\n","Validation Loss: 0.7989, Validation Accuracy: 72.73%\n","Epoch [10/15]\n","Training Loss: 0.7683, Training Accuracy: 73.41%\n","Validation Loss: 0.7858, Validation Accuracy: 73.09%\n","Epoch [11/15]\n","Training Loss: 0.7527, Training Accuracy: 73.95%\n","Validation Loss: 0.7677, Validation Accuracy: 73.78%\n","Epoch [12/15]\n","Training Loss: 0.7391, Training Accuracy: 74.46%\n","Validation Loss: 0.7756, Validation Accuracy: 73.57%\n","Epoch [13/15]\n","Training Loss: 0.7266, Training Accuracy: 74.92%\n","Validation Loss: 0.7743, Validation Accuracy: 73.62%\n","Epoch [14/15]\n","Training Loss: 0.7171, Training Accuracy: 75.21%\n","Validation Loss: 0.7534, Validation Accuracy: 74.92%\n","Epoch [15/15]\n","Training Loss: 0.7067, Training Accuracy: 75.52%\n","Validation Loss: 0.7618, Validation Accuracy: 74.15%\n","[{'seed': 42, 'alpha_init': 0.5, 'gamma_init': 0.5, 'ensemble_size': 4, 'num_epochs': 15, 'batch_size': 4, 'lr': 0.002, 'model_type': 'batchensemble', 'final_test_accuracy': 74.15, 'avg_test_accuracy': 70.97266666666668, 'final_losses': 0.7066715142940729, 'avg_losses': 0.9054848498092914}]\n","Next calculation with differences in ensemble size: ensemble size = 8\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 2.1176, Training Accuracy: 37.64%\n","Validation Loss: 1.3058, Validation Accuracy: 53.92%\n","Epoch [2/15]\n","Training Loss: 1.3054, Training Accuracy: 53.99%\n","Validation Loss: 1.0822, Validation Accuracy: 61.76%\n","Epoch [3/15]\n","Training Loss: 1.1185, Training Accuracy: 60.91%\n","Validation Loss: 0.9971, Validation Accuracy: 65.95%\n","Epoch [4/15]\n","Training Loss: 1.0305, Training Accuracy: 64.14%\n","Validation Loss: 0.9275, Validation Accuracy: 68.11%\n","Epoch [5/15]\n","Training Loss: 0.9750, Training Accuracy: 66.28%\n","Validation Loss: 0.8781, Validation Accuracy: 69.87%\n","Epoch [6/15]\n","Training Loss: 0.9397, Training Accuracy: 67.39%\n","Validation Loss: 0.9181, Validation Accuracy: 68.84%\n","Epoch [7/15]\n","Training Loss: 0.9107, Training Accuracy: 68.47%\n","Validation Loss: 0.8360, Validation Accuracy: 71.12%\n","Epoch [8/15]\n","Training Loss: 0.8890, Training Accuracy: 69.29%\n","Validation Loss: 0.8352, Validation Accuracy: 71.96%\n","Epoch [9/15]\n","Training Loss: 0.8680, Training Accuracy: 69.97%\n","Validation Loss: 0.8267, Validation Accuracy: 71.36%\n","Epoch [10/15]\n","Training Loss: 0.8510, Training Accuracy: 70.61%\n","Validation Loss: 0.8371, Validation Accuracy: 71.10%\n","Epoch [11/15]\n","Training Loss: 0.8384, Training Accuracy: 70.95%\n","Validation Loss: 0.8063, Validation Accuracy: 72.53%\n","Epoch [12/15]\n","Training Loss: 0.8244, Training Accuracy: 71.52%\n","Validation Loss: 0.8012, Validation Accuracy: 72.02%\n","Epoch [13/15]\n","Training Loss: 0.8143, Training Accuracy: 71.84%\n","Validation Loss: 0.7761, Validation Accuracy: 73.32%\n","Epoch [14/15]\n","Training Loss: 0.8030, Training Accuracy: 72.20%\n","Validation Loss: 0.7978, Validation Accuracy: 72.79%\n","Epoch [15/15]\n","Training Loss: 0.7943, Training Accuracy: 72.52%\n","Validation Loss: 0.7708, Validation Accuracy: 73.46%\n","[{'seed': 42, 'alpha_init': 0.5, 'gamma_init': 0.5, 'ensemble_size': 8, 'num_epochs': 15, 'batch_size': 8, 'lr': 0.002, 'model_type': 'batchensemble', 'final_test_accuracy': 73.46, 'avg_test_accuracy': 69.20733333333334, 'final_losses': 0.7943247568368912, 'avg_losses': 1.00533385147055}]\n"]}]},{"cell_type":"code","source":["# Test optimizer\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","output_folder = \"data\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","alpha = 1.0\n","gamma =  0.2\n","ensemble_size = 2\n","lr = 0.002\n","optimizer_type = \"adam\" # adam, sgd, ivon\n","\n","filename = optimizer_type + \"_results_\" + str(ensemble_size) + \".csv\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","output_folder = \"data\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","optimizer_file = os.path.join(output_folder, filename)\n","\n","results = train_with_params(seed=42, ensemble_size=ensemble_size, alpha_init=alpha, gamma_init=gamma, num_epochs=15, batch_size=256, lr=lr, device=device, optimizer_type=optimizer_type, mode=\"optimizer\")\n","print(results)\n","# save_results_to_csv(results, optimizer_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osl2Ek1owPZF","executionInfo":{"status":"ok","timestamp":1734715817752,"user_tz":-60,"elapsed":329449,"user":{"displayName":"Justine","userId":"10003038839395646781"}},"outputId":"510a1676-83d6-4b39-83a8-28d19234a3b8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Using device: cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Training CNN model: batchensemble\n","Epoch [1/15]\n","Training Loss: 6.0576, Training Accuracy: 29.12%\n","Validation Loss: 2.2163, Validation Accuracy: 38.12%\n","Epoch [2/15]\n","Training Loss: 2.2004, Training Accuracy: 37.15%\n","Validation Loss: 1.6784, Validation Accuracy: 43.90%\n","Epoch [3/15]\n","Training Loss: 1.7723, Training Accuracy: 42.14%\n","Validation Loss: 1.5588, Validation Accuracy: 47.02%\n","Epoch [4/15]\n","Training Loss: 1.5950, Training Accuracy: 45.91%\n","Validation Loss: 1.4795, Validation Accuracy: 47.81%\n","Epoch [5/15]\n","Training Loss: 1.4817, Training Accuracy: 48.93%\n","Validation Loss: 1.4639, Validation Accuracy: 49.37%\n","Epoch [6/15]\n","Training Loss: 1.4042, Training Accuracy: 51.13%\n","Validation Loss: 1.3190, Validation Accuracy: 53.23%\n","Epoch [7/15]\n","Training Loss: 1.3343, Training Accuracy: 53.50%\n","Validation Loss: 1.2627, Validation Accuracy: 55.57%\n","Epoch [8/15]\n","Training Loss: 1.2851, Training Accuracy: 55.09%\n","Validation Loss: 1.2191, Validation Accuracy: 56.38%\n","Epoch [9/15]\n","Training Loss: 1.2379, Training Accuracy: 56.91%\n","Validation Loss: 1.2125, Validation Accuracy: 56.53%\n","Epoch [10/15]\n","Training Loss: 1.2035, Training Accuracy: 57.99%\n","Validation Loss: 1.1811, Validation Accuracy: 57.92%\n","Epoch [11/15]\n","Training Loss: 1.1652, Training Accuracy: 59.35%\n","Validation Loss: 1.1295, Validation Accuracy: 60.02%\n","Epoch [12/15]\n","Training Loss: 1.1287, Training Accuracy: 60.61%\n","Validation Loss: 1.1375, Validation Accuracy: 59.93%\n","Epoch [13/15]\n","Training Loss: 1.1017, Training Accuracy: 61.37%\n","Validation Loss: 1.0925, Validation Accuracy: 61.33%\n","Epoch [14/15]\n","Training Loss: 1.0730, Training Accuracy: 62.63%\n","Validation Loss: 1.1259, Validation Accuracy: 60.77%\n","Epoch [15/15]\n","Training Loss: 1.0473, Training Accuracy: 63.65%\n","Validation Loss: 1.0949, Validation Accuracy: 61.51%\n","[{'ensemble_size': 2, 'num_epochs': 15, 'batch_size': 256, 'lr': 0.002, 'model_type': 'batchensemble', 'train_loss': [6.0576034127449505, 2.200388856688324, 1.7723347167579495, 1.5950416302194401, 1.4817067317816677, 1.4041663900930055, 1.334322271298389, 1.2851385249167073, 1.2379087088059406, 1.2034881492050327, 1.1651588401624136, 1.1287020390131035, 1.1017053927085838, 1.0729752721226946, 1.0472737368272276], 'val_loss': [2.216313248872757, 1.6783936589956283, 1.5587862759828568, 1.4795379787683487, 1.4639402836561204, 1.3189846754074097, 1.2627337783575059, 1.219112014770508, 1.2125295490026473, 1.1810577660799026, 1.129520872235298, 1.1374738797545434, 1.0924568966031074, 1.1258581593632697, 1.0948628649115562], 'train_acc': [29.117, 37.153, 42.138, 45.913, 48.927, 51.132, 53.5, 55.085, 56.915, 57.992, 59.349, 60.609, 61.37, 62.633, 63.651], 'val_acc': [38.12, 43.9, 47.02, 47.81, 49.37, 53.23, 55.57, 56.38, 56.53, 57.92, 60.02, 59.93, 61.33, 60.77, 61.51]}]\n"]}]}]}