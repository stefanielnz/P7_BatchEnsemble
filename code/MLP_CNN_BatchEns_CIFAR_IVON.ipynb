{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d44c71-6cd6-4558-bf91-7c0219b2969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 581\u001b[0m\n\u001b[0;32m    578\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 581\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 559\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model_type. Choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchensemble\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    558\u001b[0m cnn_models[model_type] \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 559\u001b[0m tr_losses, te_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mnist_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensemble_size_cnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m cnn_train_losses[model_type] \u001b[38;5;241m=\u001b[39m tr_losses\n\u001b[0;32m    569\u001b[0m cnn_test_accuracies[model_type] \u001b[38;5;241m=\u001b[39m te_accuracies\n",
      "Cell \u001b[1;32mIn[8], line 390\u001b[0m, in \u001b[0;36mtrain_mnist_model\u001b[1;34m(model, train_loader, test_loader, model_type, ensemble_size, num_epochs, lr)\u001b[0m\n\u001b[0;32m    388\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    389\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 390\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m##### IVON: until here inside \"with\" or ADAM: move out until here\u001b[39;00m\n\u001b[0;32m    392\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\jfast\\Documents\\Justine\\03_Studium\\Master\\Semester_3_Ausland\\DeepLearning\\Project\\Repo\\Repo_Clone\\venv_DL_clone\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jfast\\Documents\\Justine\\03_Studium\\Master\\Semester_3_Ausland\\DeepLearning\\Project\\Repo\\Repo_Clone\\venv_DL_clone\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jfast\\Documents\\Justine\\03_Studium\\Master\\Semester_3_Ausland\\DeepLearning\\Project\\Repo\\Repo_Clone\\venv_DL_clone\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Iterable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import ivon\n",
    "\n",
    "# BatchEnsemble implementation\n",
    "\n",
    "def random_sign_(tensor: torch.Tensor, prob: float = 0.5, value: float = 1.0):\n",
    "    \"\"\"\n",
    "    Randomly set elements of the input tensor to either +value or -value.\n",
    "    \"\"\"\n",
    "    sign = torch.where(torch.rand_like(tensor) < prob, 1.0, -1.0)\n",
    "    with torch.no_grad():\n",
    "        tensor.copy_(sign * value)\n",
    "\n",
    "class BatchEnsembleMixin:\n",
    "    def init_ensemble(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        ensemble_size: int,\n",
    "        alpha_init: float | None = None,\n",
    "        gamma_init: float | None = None,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.alpha_init = alpha_init\n",
    "        self.gamma_init = gamma_init\n",
    "\n",
    "        if not isinstance(self, nn.Module):\n",
    "            raise TypeError(\"BatchEnsembleMixin must be mixed with nn.Module or one of its subclasses\")\n",
    "\n",
    "        if alpha_init is None:\n",
    "            self.register_parameter(\"alpha_param\", None)\n",
    "        else:\n",
    "            self.alpha_param = self.init_scaling_parameter(alpha_init, in_features, device=device, dtype=dtype)\n",
    "            self.register_parameter(\"alpha_param\", self.alpha_param)\n",
    "\n",
    "        if gamma_init is None:\n",
    "            self.register_parameter(\"gamma_param\", None)\n",
    "        else:\n",
    "            self.gamma_param = self.init_scaling_parameter(gamma_init, out_features, device=device, dtype=dtype)\n",
    "            self.register_parameter(\"gamma_param\", self.gamma_param)\n",
    "\n",
    "        if bias:\n",
    "            self.bias_param = nn.Parameter(torch.zeros(ensemble_size, out_features, device=device, dtype=dtype))\n",
    "            self.register_parameter(\"bias_param\", self.bias_param)\n",
    "        else:\n",
    "            self.register_parameter(\"bias_param\", None)\n",
    "\n",
    "    def init_scaling_parameter(self, init_value: float, num_features: int, device=None, dtype=None):\n",
    "        param = torch.empty(self.ensemble_size, num_features, device=device, dtype=dtype)\n",
    "        if init_value < 0:\n",
    "            param.normal_(mean=1, std=-init_value)\n",
    "        else:\n",
    "            random_sign_(param, prob=init_value, value=1.0)\n",
    "        return nn.Parameter(param)\n",
    "\n",
    "    def expand_param(self, x: torch.Tensor, param: torch.Tensor):\n",
    "        num_repeats = x.size(0) // self.ensemble_size\n",
    "        expanded_param = torch.repeat_interleave(param, num_repeats, dim=0)\n",
    "        extra_dims = len(x.shape) - len(expanded_param.shape)\n",
    "        for _ in range(extra_dims):\n",
    "            expanded_param = expanded_param.unsqueeze(-1)\n",
    "        return expanded_param\n",
    "\n",
    "class BELinear(nn.Linear, BatchEnsembleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        ensemble_size: int = 1,\n",
    "        alpha_init: float | None = None,\n",
    "        gamma_init: float | None = None,\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, bias=False, device=device, dtype=dtype)\n",
    "        self.init_ensemble(in_features, out_features, ensemble_size, alpha_init, gamma_init, bias)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if self.alpha_init is not None:\n",
    "            input = input * self.expand_param(input, self.alpha_param)\n",
    "        x = F.linear(input, self.weight)\n",
    "        if self.gamma_init is not None:\n",
    "            x = x * self.expand_param(x, self.gamma_param)\n",
    "        if self.bias_param is not None:\n",
    "            x = x + self.expand_param(x, self.bias_param)\n",
    "        return x\n",
    "\n",
    "class Conv2d(nn.Conv2d, BatchEnsembleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias: bool = True,\n",
    "        padding_mode='zeros',\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        ensemble_size: int = 1,\n",
    "        alpha_init: float | None = None,\n",
    "        gamma_init: float | None = None,\n",
    "    ):\n",
    "        nn.Conv2d.__init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,  # Bias is managed by BatchEnsembleMixin\n",
    "            padding_mode=padding_mode,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.init_ensemble(\n",
    "            in_features=in_channels,\n",
    "            out_features=out_channels,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha_init,\n",
    "            gamma_init=gamma_init,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if self.alpha_init is not None:\n",
    "            input = input * self.expand_param(input, self.alpha_param)\n",
    "        x = self._conv_forward(input, self.weight, None)\n",
    "        if self.gamma_init is not None:\n",
    "            x = x * self.expand_param(x, self.gamma_param)\n",
    "        if self.bias_param is not None:\n",
    "            x = x + self.expand_param(x, self.bias_param)\n",
    "        return x\n",
    "\n",
    "class BatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps=1e-5,\n",
    "        momentum=0.1,\n",
    "        affine=False,  # We will manage affine parameters ourselves\n",
    "        track_running_stats=True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        ensemble_size: int = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_features,\n",
    "            eps=eps,\n",
    "            momentum=momentum,\n",
    "            affine=affine,\n",
    "            track_running_stats=track_running_stats,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.ensemble_size = ensemble_size\n",
    "        # Use different names to avoid conflicts with the base class\n",
    "        self.weight_be = nn.Parameter(torch.Tensor(ensemble_size, num_features))\n",
    "        self.bias_be = nn.Parameter(torch.Tensor(ensemble_size, num_features))\n",
    "        self.reset_be_parameters()\n",
    "\n",
    "    def reset_be_parameters(self):\n",
    "        nn.init.ones_(self.weight_be)\n",
    "        nn.init.zeros_(self.bias_be)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # Use the base class's forward method\n",
    "        x = super().forward(input)\n",
    "        num_repeats = x.size(0) // self.ensemble_size\n",
    "        weight = torch.repeat_interleave(self.weight_be, num_repeats, dim=0).unsqueeze(2).unsqueeze(3)\n",
    "        bias = torch.repeat_interleave(self.bias_be, num_repeats, dim=0).unsqueeze(2).unsqueeze(3)\n",
    "        x = x * weight + bias\n",
    "        return x\n",
    "\n",
    "# StandardScaler\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Based on https://gist.github.com/farahmand-m/8a416f33a27d73a149f92ce4708beb40\n",
    "    Extended with inverse_transform\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mean: torch.Tensor | None = None,\n",
    "        std: torch.Tensor | None = None,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, data: torch.Tensor):\n",
    "        reduction_axes = list(range(data.dim() - 1))\n",
    "        self.mean = torch.mean(data, dim=reduction_axes)\n",
    "        self.std = torch.std(data, dim=reduction_axes)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise RuntimeError(\"Call fit before calling transform.\")\n",
    "        scaled_mean = (data - self.mean) / (self.std + self.epsilon)\n",
    "        return scaled_mean\n",
    "\n",
    "    def inverse_transform(self, scaled_data: torch.Tensor) -> torch.Tensor:\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise RuntimeError(\"Call fit before calling inverse_transform.\")\n",
    "        unscaled_mean = scaled_data * (self.std + self.epsilon) + self.mean\n",
    "        return unscaled_mean\n",
    "\n",
    "    def fit_transform(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def to(self, target_device: torch.device):\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            self.mean = self.mean.to(target_device)\n",
    "            self.std = self.std.to(target_device)\n",
    "        return self\n",
    "\n",
    "# MLP Models for MNIST\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    #def __init__(self, input_size=28*28, hidden_size=256, num_classes=10): #MNIST\n",
    "    def __init__(self, input_size=32*32*3, hidden_size=256, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        #assert False, x.shape\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class BatchEnsembleMLP(nn.Module):\n",
    "    # def __init__(self, input_size=28*28, hidden_size=256, num_classes=10, ensemble_size=4, alpha_gamma_init=0.5): #MNIST\n",
    "    def __init__(self, input_size=32*32*3, hidden_size=256, num_classes=10, ensemble_size=4, alpha_gamma_init=0.5): #CIFAR\n",
    "        super(BatchEnsembleMLP, self).__init__()\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.fc1 = BELinear(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha_gamma_init,\n",
    "            gamma_init=alpha_gamma_init,\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = BELinear(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha_gamma_init,\n",
    "            gamma_init=alpha_gamma_init,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = BELinear(\n",
    "            hidden_size,\n",
    "            num_classes,\n",
    "            ensemble_size=ensemble_size,\n",
    "            alpha_init=alpha_gamma_init,\n",
    "            gamma_init=alpha_gamma_init,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# CNN Models\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) #MNIST\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) #CIFAR\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # self.fc = nn.Linear(64 * 7 * 7, 10) # MNIST\n",
    "        self.fc = nn.Linear(64 * 8 * 8, 10) # CIFAR\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # [batch_size, 32, 28, 28] # CIFAR [batch_size, 32, 32, 32]\n",
    "        x = self.pool(x)                        # [batch_size, 32, 14, 14] # CIFAR [batch_size, 32, 17, 17]\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # [batch_size, 64, 14, 14] # CIFAR [batch_size, 64, 17, 17]\n",
    "        x = self.pool(x)                        # [batch_size, 64, 7, 7]   # CIFAR [batch_size, 64, 8, 8]\n",
    "        x = x.view(x.size(0), -1)               # [batch_size, 64*7*7]     # CIFAR [batch_size, 64*8*8] \n",
    "        x = self.fc(x)                          # [batch_size, 10]         # CIFAR [batch_size, 10]\n",
    "        return x\n",
    "\n",
    "class BatchEnsembleCNN(nn.Module):\n",
    "    def __init__(self, ensemble_size=4, alpha_gamma_init=0.5):\n",
    "        super(BatchEnsembleCNN, self).__init__()\n",
    "        self.ensemble_size = ensemble_size\n",
    "        # MNIST\n",
    "        # self.conv1 = Conv2d(1, 32, kernel_size=3, padding=1,\n",
    "        #                     ensemble_size=ensemble_size,\n",
    "        #                     alpha_init=alpha_gamma_init,\n",
    "        #                     gamma_init=alpha_gamma_init)\n",
    "        # CIFAR\n",
    "        self.conv1 = Conv2d(3, 32, kernel_size=3, padding=1,\n",
    "                            ensemble_size=ensemble_size,\n",
    "                            alpha_init=alpha_gamma_init,\n",
    "                            gamma_init=alpha_gamma_init)\n",
    "        self.bn1 = BatchNorm2d(32, ensemble_size=ensemble_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3, padding=1,\n",
    "                            ensemble_size=ensemble_size,\n",
    "                            alpha_init=alpha_gamma_init,\n",
    "                            gamma_init=alpha_gamma_init)\n",
    "        self.bn2 = BatchNorm2d(64, ensemble_size=ensemble_size)\n",
    "        # MNIST\n",
    "        # self.fc = BELinear(64 * 7 * 7, 10,\n",
    "        #                    ensemble_size=ensemble_size,\n",
    "        #                    alpha_init=alpha_gamma_init,\n",
    "        #                    gamma_init=alpha_gamma_init)\n",
    "        # CIFAR\n",
    "        self.fc = BELinear(64 * 8 * 8, 10,\n",
    "                           ensemble_size=ensemble_size,\n",
    "                           alpha_init=alpha_gamma_init,\n",
    "                           gamma_init=alpha_gamma_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))   # [batch_size * ensemble_size, 32, 28, 28] # CIFAR [batch_size * ensemble_size, 32, 32, 32]\n",
    "        x = self.pool(x)                         # [batch_size * ensemble_size, 32, 14, 14] # CIFAR [batch_size * ensemble_size, 32, 17, 17]\n",
    "        x = self.relu(self.bn2(self.conv2(x)))   # [batch_size * ensemble_size, 64, 14, 14] # CIFAR [batch_size * ensemble_size, 64, 17, 17]\n",
    "        x = self.pool(x)                         # [batch_size * ensemble_size, 64, 7, 7]   # CIFAR [batch_size * ensemble_size, 64, 8, 8]\n",
    "        x = x.view(x.size(0), -1)                # [batch_size * ensemble_size, 64*7*7]     # CIFAR [batch_size * ensemble_size, 64*8*8] \n",
    "        x = self.fc(x)                           # [batch_size * ensemble_size, 10]         # CIFAR [batch_size * ensemble_size, 10]\n",
    "        return x\n",
    "\n",
    "# Training and visualization functions\n",
    "\n",
    "def train_mnist_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    model_type=\"simple\",\n",
    "    ensemble_size=1,\n",
    "    num_epochs=5,\n",
    "    lr=0.01,\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    #####\n",
    "    # optimizer = ivon.IVON(model.parameters(), lr=lr, ess=len(train_loader), weight_decay=1e-4, beta1=0.9, beta2=0.999)\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            ##\n",
    "            # with optimizer.sampled_params(train=True):\n",
    "                ### IVON: move inside with from here or ADAM: move one step back from here\n",
    "            optimizer.zero_grad()\n",
    "            if model_type == \"batchensemble\":\n",
    "                images = images.repeat(ensemble_size, 1, 1, 1)\n",
    "                labels = labels.repeat(ensemble_size)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            ##### IVON: until here inside \"with\" or ADAM: move out until here\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                if model_type == \"batchensemble\":\n",
    "                    images = images.repeat(ensemble_size, 1, 1, 1)\n",
    "                    outputs = model(images)\n",
    "                    outputs = outputs.view(ensemble_size, -1, 10)\n",
    "                    outputs = outputs.mean(dim=0)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], {model_type.capitalize()} Model Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "def plot_training_results(train_losses, test_accuracies, ax, model_type=\"simple\"):\n",
    "    ax.plot(train_losses, label=\"Train loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"{model_type.capitalize()} Training Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(test_accuracies, label=\"Test Accuracy\", color=\"orange\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\")\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_parameters(model, ax, model_name=\"Model\"):\n",
    "    # Flatten all parameters, then plot in descending order\n",
    "    params = []\n",
    "    tags = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" in name:\n",
    "            tag = \"bias\"\n",
    "        elif \"alpha\" in name:\n",
    "            tag = \"alpha\"\n",
    "        elif \"gamma\" in name:\n",
    "            tag = \"gamma\"\n",
    "        elif \"weight\" in name:\n",
    "            tag = \"weight\"\n",
    "        else:\n",
    "            tag = \"unknown\"\n",
    "        params.append(param.detach().cpu().flatten())\n",
    "        tags += [tag] * param.numel()\n",
    "    params = torch.cat(params).numpy()\n",
    "    sns.histplot(x=params, hue=tags, ax=ax, kde=True, bins=50)\n",
    "    ax.set_xlabel(\"Parameter value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Parameter distribution of {model_name}\")\n",
    "\n",
    "# Main function\n",
    "\n",
    "def main():\n",
    "    # Set random seed\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Hyperparameters\n",
    "    ensemble_size = 4\n",
    "    alpha_gamma_init = 0.5\n",
    "    num_epochs = 5\n",
    "    batch_size = 128\n",
    "    lr = 0.01\n",
    "\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # # Load MNIST dataset\n",
    "    # train_dataset = datasets.MNIST(root='./data', train=True,\n",
    "    #                                download=True, transform=transform)\n",
    "    # test_dataset = datasets.MNIST(root='./data', train=False,\n",
    "    #                               download=True, transform=transform)\n",
    "    # # CIFAR10\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Adjust batch size to be multiple of ensemble_size\n",
    "    assert batch_size % ensemble_size == 0, \"Batch size must be divisible by ensemble size.\"\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Build MLP models\n",
    "    mlp_models = {}\n",
    "    mlp_train_losses = {}\n",
    "    mlp_test_accuracies = {}\n",
    "    model_types = [\"simple\", \"batchensemble\"]\n",
    "    for model_type in model_types:\n",
    "        if model_type == \"simple\":\n",
    "            model = MLP()\n",
    "            ensemble_size_mlp = 1\n",
    "        elif model_type == \"batchensemble\":\n",
    "            model = BatchEnsembleMLP(\n",
    "                ensemble_size=ensemble_size,\n",
    "                alpha_gamma_init=alpha_gamma_init\n",
    "            )\n",
    "            ensemble_size_mlp = ensemble_size\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'simple' or 'batchensemble'\")\n",
    "        mlp_models[model_type] = model\n",
    "        tr_losses, te_accuracies = train_mnist_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            model_type=model_type,\n",
    "            ensemble_size=ensemble_size_mlp,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "        )\n",
    "        mlp_train_losses[model_type] = tr_losses\n",
    "        mlp_test_accuracies[model_type] = te_accuracies\n",
    "\n",
    "    # Plot MLP results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        plot_training_results(mlp_train_losses[model_type], mlp_test_accuracies[model_type], axs[0, i], model_type=model_type)\n",
    "        plot_parameters(mlp_models[model_type], axs[1, i], model_name=f\"{model_type.capitalize()} MLP\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Build CNN models\n",
    "    cnn_models = {}\n",
    "    cnn_train_losses = {}\n",
    "    cnn_test_accuracies = {}\n",
    "    model_types = [\"simple\", \"batchensemble\"]\n",
    "    for model_type in model_types:\n",
    "        if model_type == \"simple\":\n",
    "            model = SimpleCNN()\n",
    "            ensemble_size_cnn = 1\n",
    "        elif model_type == \"batchensemble\":\n",
    "            model = BatchEnsembleCNN(\n",
    "                ensemble_size=ensemble_size,\n",
    "                alpha_gamma_init=alpha_gamma_init\n",
    "            )\n",
    "            ensemble_size_cnn = ensemble_size\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'simple' or 'batchensemble'\")\n",
    "        cnn_models[model_type] = model\n",
    "        tr_losses, te_accuracies = train_mnist_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            model_type=model_type,\n",
    "            ensemble_size=ensemble_size_cnn,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "        )\n",
    "        cnn_train_losses[model_type] = tr_losses\n",
    "        cnn_test_accuracies[model_type] = te_accuracies\n",
    "\n",
    "    # Plot CNN results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        plot_training_results(cnn_train_losses[model_type], cnn_test_accuracies[model_type], axs[0, i], model_type=model_type)\n",
    "        plot_parameters(cnn_models[model_type], axs[1, i], model_name=f\"{model_type.capitalize()} CNN\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_DL_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
